{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Deforestation Analysis in Nepal using Satellite Imagery and Deep Learning\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project aims to build and compare deep learning models for semantic segmentation to identify and map forest cover and deforestation in Nepal using publicly available satellite data. The goal is not only to create an accurate map but also to understand the end-to-end workflow of a geospatial deep learning project, from foundational data analysis to training and evaluating advanced neural network architectures like the U-Net.\n",
        "\n",
        "**The core challenge:** Can we train a model on recent, high-resolution Sentinel-2 satellite imagery to predict historical forest change categories derived from the Hansen Global Forest Change dataset? This would create a powerful tool for near-real-time monitoring."
      ],
      "metadata": {
        "id": "GFahsF2sD2Vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mi4M38iOHnch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup: Installing Libraries and Authenticating\n",
        "Preparing our Google Colab environment by installing the necessary Python libraries for accessing Google Earth Engine (`earthengine-api`) and for interactive mapping (`geemap`). We also authenticate our session to grant access to the GEE platform.\n"
      ],
      "metadata": {
        "id": "WUARGmMIEgyV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYBLq1JG-vtg"
      },
      "outputs": [],
      "source": [
        "!pip install -q earthengine-api geemap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tcpSKUt_aZt"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import geemap\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8k00z9RVdRw"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "\n",
        "# Initialize project ID from Colab Secrets\n",
        "try:\n",
        "    project_id = userdata.get('GCP_PROJECT_ID')\n",
        "    ee.Initialize(project=project_id)\n",
        "    print(\"Environment is set up and Earth Engine is initialized!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "v_tEl8tsHrrV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-XfSF1oeF7d"
      },
      "source": [
        "## 1: Foundational Analysis with Google Earth Engine\n",
        "\n",
        "Before diving into deep learning, it is crucial to understand our data and establish a baseline. In this phase, we use the Google Earth Engine Python API to perform a \"classical\" remote sensing analysis of forest change across Nepal.\n",
        "\n",
        "### 1.1: Defining the Area of Interest (AOI) and Data\n",
        "We begin by loading the Hansen Global Forest Change dataset, which provides a global, 30-meter resolution record of tree cover, loss, and gain since the year 2000. The most recent one being <i>global_forest_change_2024_v1_12</i>.\n",
        "\n",
        "\n",
        "We define our AOI as the country of Nepal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqOws2jqXzds"
      },
      "outputs": [],
      "source": [
        "countries = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')\n",
        "nepal_aoi = countries.filter(ee.Filter.eq('country_na', 'Nepal'))\n",
        "\n",
        "gfc = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')\n",
        "\n",
        "tree_cover_2000 = gfc.select(['treecover2000'])\n",
        "\n",
        "# Define visualization parameters - pixels with 0% cover as light yellow and 100% cover as dark green.\n",
        "vis_params = {\n",
        "    'bands': ['treecover2000'],\n",
        "    'min': 0,\n",
        "    'max': 100,\n",
        "    'palette': ['#ffffd9', '#edf8b1', '#c7e9b4', '#7fcdbb', '#41b6c4', '#1d91c0', '#225ea8', '#0c2c84']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAMKIdF-ZXkv"
      },
      "source": [
        "### 1.2: Visualizing Baseline Forest Cover and Loss\n",
        "\n",
        "A visual inspection - creating an interactive map to visualize two key layers:\n",
        "1.  **Tree Cover in 2000:** Our baseline, showing the extent of forests at the start of our analysis period.\n",
        "2.  **Forest Loss (2000-2023):** An overlay highlighting all pixels that experienced deforestation during this period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqeV6q-kZU4m"
      },
      "outputs": [],
      "source": [
        "map_nepal = geemap.Map()\n",
        "map_nepal.centerObject(nepal_aoi, 7) # 7 is the zoom level\n",
        "\n",
        "# Add the tree cover layer to the map, clipping it to Nepal's borders\n",
        "map_nepal.addLayer(\n",
        "    tree_cover_2000.clip(nepal_aoi), # Clip the image to our AOI\n",
        "    vis_params,                     # Apply our visualization style\n",
        "    'Tree Cover in 2000'            # Name the layer\n",
        ")\n",
        "\n",
        "# Add the Nepal boundary to the map for context\n",
        "map_nepal.addLayer(nepal_aoi.style(**{'color': 'black', 'fillColor': '00000000'}), {}, 'Nepal Border')\n",
        "\n",
        "map_nepal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzESSM_fdDfI"
      },
      "outputs": [],
      "source": [
        "# Select the forest loss band - this band is binary: 1 for loss, 0 for no loss.\n",
        "forest_loss = gfc.select(['loss'])\n",
        "\n",
        "#  .selfMask() to make all '0' pixels transparent ensures that we only see the pixels where loss actually occurred.\n",
        "forest_loss_masked = forest_loss.selfMask()\n",
        "\n",
        "# Define visualization parameters for the loss layer\n",
        "loss_vis_params = {\n",
        "    'palette': ['#FF0000'] # Bright Red\n",
        "}\n",
        "\n",
        "# Add the masked loss layer to our existing map object\n",
        "map_nepal.addLayer(\n",
        "    forest_loss_masked.clip(nepal_aoi), # Clip the layer to our AOI\n",
        "    loss_vis_params,                    # Apply the red color\n",
        "    'Forest Loss (2000-2023)'           # Name the layer\n",
        ")\n",
        "\n",
        "map_nepal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJROh3W4eQPF"
      },
      "source": [
        "### 1.3 Quantification of National Forest Change (2000-2023)\n",
        "\n",
        "Visuals are insightful, but for analysis, we need numbers. We perform a quantitative analysis to calculate the total area of forest cover in 2000 and the total area of forest lost and gained up to 2023. This gives us a high-level summary of Nepal's net forest change.\n",
        "\n",
        "<i> We define \"forest\" as any area with a tree canopy cover of 30% or more. </i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhfeTndyebLq"
      },
      "outputs": [],
      "source": [
        "gfc = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')\n",
        "\n",
        "# forest threshold (in percentage).\n",
        "CANOPY_COVER_THRESHOLD = 30\n",
        "\n",
        "# the baseline tree cover and create a forest mask\n",
        "tree_cover_2000 = gfc.select(['treecover2000'])\n",
        "is_forest_2000 = tree_cover_2000.gte(CANOPY_COVER_THRESHOLD) # gte = Greater Than or Equal to\n",
        "\n",
        "# calculate the area of the forest - ee.Image.pixelArea() creates an image where each pixel's value is its area in square meters.\n",
        "pixel_area_m2 = ee.Image.pixelArea()\n",
        "\n",
        "# multiply the area image by our mask.\n",
        "forest_area_m2 = is_forest_2000.multiply(pixel_area_m2)\n",
        "\n",
        "# sum the area over the entire AOI (Nepal) - .reduceRegion() is how we get statistics from an image for a specific region.\n",
        "area_stats = forest_area_m2.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=nepal_aoi.geometry(),\n",
        "    scale=30,  # The native resolution of the Hansen dataset is 30 meters\n",
        "    maxPixels=1e10 # Use a large number to ensure the calculation doesn't time out\n",
        ")\n",
        "\n",
        "# extract the result - from a dictionary | convert to km2\n",
        "forest_area_sq_m = area_stats.get('treecover2000').getInfo()\n",
        "forest_area_sq_km = forest_area_sq_m / 1e6\n",
        "\n",
        "print(f\"Using a {CANOPY_COVER_THRESHOLD}% canopy cover threshold...\")\n",
        "print(f\"Total forest area in Nepal (Year 2000): {forest_area_sq_km:,.2f} sq km\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIglXIz8fLuW"
      },
      "outputs": [],
      "source": [
        "# Quantify Total Forest Loss Area (2000-2023) and Calculate Percentage\n",
        "gfc = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')\n",
        "\n",
        "is_forest_2000 = gfc.select(['treecover2000']).gte(CANOPY_COVER_THRESHOLD)\n",
        "\n",
        "# select the loss band and create a mask of where loss occurred\n",
        "has_loss = gfc.select(['loss']).eq(1)\n",
        "\n",
        "# combine the two conditions: was forest in 2000 AND has loss since then\n",
        "actual_forest_loss = is_forest_2000.And(has_loss)\n",
        "\n",
        "# use the pixel area image to calculate the area of the loss\n",
        "pixel_area_m2 = ee.Image.pixelArea()\n",
        "loss_area_m2 = actual_forest_loss.multiply(pixel_area_m2)\n",
        "\n",
        "# sum the loss area over the entire AOI (Nepal)\n",
        "loss_stats = loss_area_m2.reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=nepal_aoi.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e10\n",
        ")\n",
        "\n",
        "loss_area_sq_m = loss_stats.get('treecover2000').getInfo()\n",
        "loss_area_sq_km = loss_area_sq_m / 1e6\n",
        "\n",
        "print(f\"Total forest area lost in Nepal (2000-2024): {loss_area_sq_km:,.2f} sq km\")\n",
        "\n",
        "# calculate and print the percentage loss\n",
        "percentage_loss = (loss_area_sq_km / forest_area_sq_km) * 100\n",
        "print(f\"This represents a {percentage_loss:.2f}% loss of the year 2000 forest cover.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7tIlgtAgKrj"
      },
      "source": [
        "### 1.4 Time-Series Analysis of Annual Forest Loss\n",
        "\n",
        "To understand the dynamics of deforestation, we analyze the `lossyear` band of the Hansen dataset. This allows us to create a time-series plot showing exactly how much forest area was lost each year, revealing trends and highlighting years with significant deforestation events.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGewgo2rgRCE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- GEE Computation ---\n",
        "\n",
        "gfc = ee.Image('UMD/hansen/global_forest_change_2023_v1_11')\n",
        "\n",
        "# prepare the data for the calculation\n",
        "loss_year = gfc.select(['lossyear'])\n",
        "is_forest_2000 = gfc.select(['treecover2000']).gte(CANOPY_COVER_THRESHOLD)\n",
        "pixel_area_m2 = ee.Image.pixelArea()\n",
        "\n",
        "# First band is the value to sum (area), second is the group key (year).\n",
        "area_by_year = pixel_area_m2.addBands(loss_year).updateMask(is_forest_2000)\n",
        "\n",
        "# Perform a 'grouped reducer' to sum the area for each year\n",
        "print(\"Running calculation on Earth Engine servers... (This may take a minute)\")\n",
        "loss_stats_by_year = area_by_year.reduceRegion(\n",
        "    # *** AND THE SECOND PART OF THE FIX IS HERE ***\n",
        "    reducer=ee.Reducer.sum().group(\n",
        "        groupField=1,  # Group by the second band ('lossyear')\n",
        "        groupName='year',\n",
        "    ),\n",
        "    geometry=nepal_aoi.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e10\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Fetch the data from GEE and convert it to a Pandas DataFrame\n",
        "stats_list = loss_stats_by_year.get('groups').getInfo()\n",
        "\n",
        "# Process the raw list of dictionaries from GEE into a clean format\n",
        "loss_data = []\n",
        "for item in stats_list:\n",
        "    year = int(item['year']) + 2000\n",
        "    # The reducer gives a 'sum' of the first band (pixel area). Convert from m^2 to km^2\n",
        "    area_sq_km = item['sum'] / 1e6\n",
        "    loss_data.append({'Year': year, 'Loss (sq km)': area_sq_km})\n",
        "\n",
        "df_loss = pd.DataFrame(loss_data)\n",
        "df_loss = df_loss.sort_values(by='Year')\n",
        "\n",
        "# Create the plot\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "sns.barplot(x='Year', y='Loss (sq km)', data=df_loss, ax=ax, color='crimson')\n",
        "\n",
        "ax.set_title('Annual Forest Loss in Nepal (2001-2023)', fontsize=16)\n",
        "ax.set_xlabel('Year', fontsize=12)\n",
        "ax.set_ylabel('Area of Forest Loss (kmÂ²)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b9e2HaDrWlI"
      },
      "source": [
        "### 1.5 Summary of National Forest Change in Nepal (2000-2024)\n",
        "\n",
        "The quantitative analysis reveals the high-level dynamics of Nepal's forests over the last two decades. The key findings are:\n",
        "\n",
        "*   **Initial Forest Area (2000):** A substantial portion of the country was forested at the beginning of the period.\n",
        "*   **Total Forest Loss:** A significant area of this original forest was lost.\n",
        "*   **Total Forest Gain:** Simultaneously, an impressive area of new forest was gained, often on land that was not forested in 2000.\n",
        "*   **Net Change:** The final net change shows the overall trend. For Nepal, this analysis confirms the trend of **net reforestation** reported in many studies, where forest gain has outpaced forest loss over this period, largely due to successful community forestry initiatives.\n",
        "\n",
        "This national-level summary provides the crucial context for our deep learning models, which will attempt to map these same categories at a finer, patch-based scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxCkD54SrTUj"
      },
      "outputs": [],
      "source": [
        "# Analyze forest gain\n",
        "# Select the gain band and create a mask where gain has occured\n",
        "gfc = ee.Image(\"UMD/hansen/global_forest_change_2024_v1_12\")\n",
        "has_gain = gfc.select(['gain']).eq(1)\n",
        "\n",
        "# Use the pixel area to calculate the area of gain\n",
        "pixel_area_m2 = ee.Image.pixelArea()\n",
        "gain_area_m2 = has_gain.multiply(pixel_area_m2)\n",
        "\n",
        "# Sum the gain area over entire Nepal AOI\n",
        "gain_stats = gain_area_m2.reduceRegion(\n",
        "    reducer = ee.Reducer.sum(),\n",
        "    geometry = nepal_aoi.geometry(),\n",
        "    scale = 30,\n",
        "    maxPixels = 1e10\n",
        ")\n",
        "\n",
        "gain_area_sq_m = gain_stats.get('gain').getInfo()\n",
        "gain_area_sq_km = gain_area_sq_m / 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UBybFOSL53F"
      },
      "outputs": [],
      "source": [
        "print(f\"Initial Forest Area (2000): {forest_area_sq_km:,.2f} sq km\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Total Forest Loss (-):      {loss_area_sq_km:,.2f} sq km\")\n",
        "print(f\"Total Forest Gain (+):      {gain_area_sq_km:,.2f} sq km\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "net_change_sq_km = gain_area_sq_km - loss_area_sq_km\n",
        "final_forest_area_sq_km = forest_area_sq_km + net_change_sq_km\n",
        "net_change_percentage = (net_change_sq_km / forest_area_sq_km) * 100\n",
        "\n",
        "print(f\"Net Change in Forest Area: {net_change_sq_km:,.2f} sq km\")\n",
        "print(f\"Final Forest Area (2023):  {final_forest_area_sq_km:,.2f} sq km\")\n",
        "\n",
        "if net_change_sq_km > 0:\n",
        "    print(f\"\\nOverall, Nepal experienced a {net_change_percentage:.2f}% NET GAIN in forest cover.\")\n",
        "else:\n",
        "    print(f\"\\nOverall, Nepal experienced a {abs(net_change_percentage):.2f}% NET LOSS in forest cover.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaNgAhCR1CP"
      },
      "source": [
        "### 1.6 Regional Analysis For FarWest Region\n",
        "\n",
        "To validate our workflow and explore regional differences, we focus our analysis on a specific sub-region. Due to the administrative boundaries available in the stable `FAO/GAUL/2015` dataset, we use the \"Far Western\" Region.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq2lCl65MZyr"
      },
      "outputs": [],
      "source": [
        "# Defining FarWest Region as the new AOI\n",
        "admin_lvl_1 = ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
        "farwest_aoi = admin_lvl_1.filter(ee.Filter.And(\n",
        "    ee.Filter.eq('ADM0_NAME', 'Nepal'),\n",
        "    ee.Filter.eq('ADM1_NAME', 'Far Western')\n",
        "))\n",
        "\n",
        "# Running the entire calculation using the AOI\n",
        "gfc = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')\n",
        "pixel_area_m2 = ee.Image.pixelArea()\n",
        "\n",
        "# Defining forest and loss/gain bands\n",
        "is_forest_2000 = gfc.select(['treecover2000']).gte(CANOPY_COVER_THRESHOLD)\n",
        "has_loss = gfc.select(['loss']).eq(1)\n",
        "has_gain = gfc.select(['gain']).eq(1)\n",
        "actual_forest_loss = is_forest_2000.And(has_loss)\n",
        "# actual_forest_gain = is_forest_2000.And(has_gain)\n",
        "\n",
        "# Calculating areas with FarWest AOI geometry\n",
        "farwest_base_area = is_forest_2000.multiply(pixel_area_m2).reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=farwest_aoi.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels = 1e9\n",
        ").get('treecover2000').getInfo() / 1e6\n",
        "\n",
        "farwest_loss_area = actual_forest_loss.multiply(pixel_area_m2).reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry = farwest_aoi.geometry(),\n",
        "    scale = 30,\n",
        "    maxPixels = 1e9\n",
        ").get('treecover2000').getInfo() / 1e6\n",
        "\n",
        "\n",
        "farwest_gain_area = has_gain.multiply(pixel_area_m2).reduceRegion(\n",
        "    reducer=ee.Reducer.sum(),\n",
        "    geometry=farwest_aoi.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e9\n",
        ").get('gain').getInfo() / 1e6\n",
        "\n",
        "print(farwest_base_area)\n",
        "print(farwest_loss_area)\n",
        "print(farwest_gain_area)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awFWpuOQT7wJ"
      },
      "outputs": [],
      "source": [
        "# Summary of change\n",
        "print(\"\\n--- Summary of Forest Change in FarWest Region (2000-2024) ---\")\n",
        "print(f\"Initial Forest Area (2000): {farwest_base_area:,.2f} sq km\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Total Forest Loss (-):      {farwest_loss_area:,.2f} sq km\")\n",
        "print(f\"Total Forest Gain (+):      {farwest_gain_area:,.2f} sq km\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "farwest_net_change = farwest_gain_area - farwest_loss_area\n",
        "farwest_final_area = farwest_base_area + farwest_net_change\n",
        "farwest_net_percentage = (farwest_net_change / farwest_base_area) * 100\n",
        "\n",
        "print(f\"Net Change in Forest Area: {farwest_net_change:,.2f} sq km\")\n",
        "print(f\"Final Forest Area (2023):  {farwest_final_area:,.2f} sq km\")\n",
        "print(f\"Net Percentage Change:     {farwest_net_percentage:+.2f}%\")\n",
        "\n",
        "if farwest_net_percentage > 0:\n",
        "    print(\"\\nFinding: Farwest Region shows a strong net gain in forest cover, aligning with the NASA case study.\")\n",
        "else:\n",
        "    print(\"\\nFinding: Farwest Region shows a net loss in forest cover overall in this period.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK-7xXaCT827"
      },
      "outputs": [],
      "source": [
        "# Plotting the findings in farwest map\n",
        "\n",
        "map_farwest = geemap.Map()\n",
        "map_farwest.centerObject(farwest_aoi, 8)\n",
        "map_farwest.addLayer(is_forest_2000.selfMask().clip(farwest_aoi),{'palette': 'green'}, 'Forest Cover 2000')\n",
        "map_farwest.addLayer(actual_forest_loss.selfMask().clip(farwest_aoi),{'palette':'red'}, 'Forest Loss')\n",
        "map_farwest.addLayer(has_gain.selfMask().clip(farwest_aoi),{'palette':'blue'}, 'Forest Gain')\n",
        "map_farwest.addLayer(farwest_aoi.style(**{'color': 'black', 'fillColor': '00000000'}), {}, 'FarWest Border')\n",
        "\n",
        "map_farwest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: Building a Baseline Deep Learning Model (SimpleFCN)\n",
        "\n",
        "With a solid understanding of the data from our foundational analysis, we now transition to the core of the project: building a deep learning model. In this phase, we will construct a simple but effective baseline model to serve as a benchmark for more advanced architectures.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YKJkvl0RXNbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1: Preparing the Training Data Sources\n",
        "\n",
        "Our deep learning model will learn the relationship between a modern satellite image and our historical forest change labels.\n",
        "*   **Input Image (X):** We use recent (late 2023), cloud-free Sentinel-2 imagery, which provides 10-meter resolution data across multiple spectral bands (Red, Green, Blue, and Near-Infrared).\n",
        "*   **Label Mask (y):** We use the Hansen dataset to create a single \"ground truth\" layer with three distinct classes: `0` for Stable Non-Forest, `1` for Stable Forest (since 2000), and `2` for Deforested Land (forest in 2000 that was later lost)."
      ],
      "metadata": {
        "id": "cpO5yChiXpmF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO6S0vafdCRz"
      },
      "source": [
        "Mount Drive & Define Data Sources\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_OMAmzFYnr5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4T6BR7udIzj"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "print(\"Drive mounted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzZXvoo-dvPr"
      },
      "outputs": [],
      "source": [
        "PROJECT_FOLDER = '/content/drive/My Drive/PyTorch_Forest_Project'\n",
        "if not os.path.exists(PROJECT_FOLDER):\n",
        "    os.makedirs(PROJECT_FOLDER)\n",
        "    print(f\"Created project folder: {PROJECT_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl5SR3Ujd0p_"
      },
      "outputs": [],
      "source": [
        "# Image Source: A recent, cloud-free Sentinel-2 mosaic\n",
        "\n",
        "s2_image = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "              .filterBounds(nepal_aoi)\n",
        "              .filterDate('2024-10-01', '2024-12-31') # post-monsoon to minimize clouds\n",
        "              .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
        "              .median() # median() is combine images and remove clouds/shadows\n",
        "              .select(['B4', 'B3', 'B2', 'B8'])) # Red, Green, Blue, Near-Infrared (NIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UocgOu5egRr"
      },
      "outputs": [],
      "source": [
        "# Label Source with Hansen Dataset\n",
        "\n",
        "gfc = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')\n",
        "\n",
        "# For our label, let's create a single-band image: 0=Non-Forest, 1=Forest, 2=Lost Forest\n",
        "is_forest_2000 = gfc.select('treecover2000').gte(30)\n",
        "has_loss = gfc.select('loss').eq(1)\n",
        "\n",
        "\n",
        "# Create the label image: Start with non-forest (0), add forest (1), then overwrite with loss (2)\n",
        "# .where(condition, value) : if-statement for each pixel\n",
        "label = (ee.Image(0)\n",
        "         .where(is_forest_2000, 1)\n",
        "         .where(is_forest_2000.And(has_loss), 2)\n",
        "         .rename('label')) # Rename the band to 'label' for clarity\n",
        "\n",
        "print(\"\\nDefined our Sentinel-2 image source and created our categorical label image.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhWgctNegZWX"
      },
      "source": [
        "### 2.2: Exporting Labeled Image Patches\n",
        "\n",
        "We cannot process the entire country at once. Instead, we generate thousands of random sample points across Nepal. For each point, we export a small 64x64 pixel \"chip\" of both the Sentinel-2 input image and the corresponding label mask. These chips are saved as GeoTIFF files to Google Drive, forming the raw dataset for our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AYlzBvvgbwi"
      },
      "outputs": [],
      "source": [
        "# Define Sampling and Export Parameters\n",
        "NUM_SAMPLES = 5000\n",
        "PATCH_SIZE = 64     # The dimension (in pixels) of the square chips to export\n",
        "SCALE = 10          # The resolution (in meters) to export at. Sentinel-2 is 10m.\n",
        "START_INDEX = 4999  # Use 0 for starting - adjust if interruption occurs from last file\n",
        "\n",
        "# Base filename for our output files\n",
        "FILE_PREFIX = 'nepal_forest_patch'\n",
        "\n",
        "# Generate Random Points for Sampling\n",
        "sample_points = ee.FeatureCollection.randomPoints(\n",
        "    region=nepal_aoi.geometry(),\n",
        "    points=NUM_SAMPLES,\n",
        "    seed=42  # seed for reproducibility\n",
        ")\n",
        "print(f\"Generated {NUM_SAMPLES} random sample points across Nepal.\")\n",
        "\n",
        "# Combine Image and Label into a Single Export Image - stack our bands: [B4, B3, B2, B8, label]. All will be in one file.\n",
        "export_image = s2_image.addBands(label).toFloat() # toFloat() for compatibility\n",
        "\n",
        "# Launch the Export Tasks -> GEE Tasks tab to monitor\n",
        "print(\"\\nStarting the export process...\")\n",
        "print(\"Go to https://code.earthengine.google.com/ and check the 'Tasks' tab to run them.\")\n",
        "\n",
        "# Create a list of all features to avoid repeatedly calling .toList()\n",
        "points_list = sample_points.toList(NUM_SAMPLES)\n",
        "\n",
        "for i in range(START_INDEX, NUM_SAMPLES):\n",
        "    point = ee.Feature(points_list.get(i))\n",
        "\n",
        "    # Create a unique filename for each patch\n",
        "    filename = f'{FILE_PREFIX}_{i:04d}'\n",
        "\n",
        "    # Define the export task for this specific patch\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=export_image,\n",
        "        description=f'GeoTIFF_Export_{filename}',\n",
        "        folder='PyTorch_Forest_Project', # The folder we created in Google Drive\n",
        "        fileNamePrefix=filename,\n",
        "        region=point.geometry().buffer(PATCH_SIZE * SCALE / 2, 1).bounds(),\n",
        "        scale=SCALE,\n",
        "        fileFormat='GeoTIFF'\n",
        "    )\n",
        "\n",
        "    task.start()\n",
        "\n",
        "print(f\"\\nSuccessfully LAUNCHED {NUM_SAMPLES} export tasks as GeoTIFF files.\")\n",
        "print(\"Please go to the GEE Tasks tab to monitor and run them. This will take some time.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3: Building the PyTorch Data Pipeline\n",
        "\n",
        "To feed data to our model efficiently, we create a custom PyTorch `Dataset` class. This class handles the logic for finding our GeoTIFF files, reading them, performing necessary pre-processing (like center-cropping to ensure uniform size), and converting them into PyTorch tensors. We then wrap this `Dataset` in a `DataLoader` to handle batching, shuffling, and parallel data loading."
      ],
      "metadata": {
        "id": "vIuH41MqYI-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a49umNtyn1He"
      },
      "outputs": [],
      "source": [
        "!pip install -q rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPnWuJiWB0SR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI_0ZYnOB36x"
      },
      "outputs": [],
      "source": [
        "class ForestDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for our forest GeoTIFF patches.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, target_size = 64):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the GeoTIFF images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.target_size = target_size\n",
        "        # Create a list of all .tif files in the directory\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.tif')]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches the sample at the given index.\n",
        "        This is where we read the file, separate image and label, and convert to tensors.\n",
        "        \"\"\"\n",
        "        # Construct the full file path\n",
        "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
        "\n",
        "        # Read the GeoTIFF file using rasterio\n",
        "        with rasterio.open(img_path) as dataset:\n",
        "            # .read() loads all bands into a NumPy array with shape (bands, height, width) - (5, 64, 64)\n",
        "            numpy_array = dataset.read()\n",
        "\n",
        "        # Enforce uniform size with a center crop - Get the current height and width of the loaded image\n",
        "        _, h, w = numpy_array.shape\n",
        "\n",
        "        # If the image is larger than our target (assume always >= target)\n",
        "        if h > self.target_size or w > self.target_size:\n",
        "            top = (h - self.target_size) // 2\n",
        "            left = (w - self.target_size) // 2\n",
        "            numpy_array = numpy_array[:, top:top + self.target_size, left:left + self.target_size]\n",
        "\n",
        "\n",
        "        # Separate the input image (X) and the label mask (y)\n",
        "        # The first 4 bands are our Sentinel-2 data (B4, B3, B2, B8)\n",
        "        image_numpy = numpy_array[:4, :, :]\n",
        "        # The 5th band is our label mask (0, 1, or 2)\n",
        "        label_numpy = numpy_array[4, :, :]\n",
        "\n",
        "        #  Convert NumPy arrays to PyTorch Tensors - Input images should be float for model computations\n",
        "        image_tensor = torch.from_numpy(image_numpy).to(torch.float32)\n",
        "        # Labels for classification should be long integers\n",
        "        label_tensor = torch.from_numpy(label_numpy).to(torch.long)\n",
        "\n",
        "        return image_tensor, label_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4: Designing the Baseline CNN Model (SimpleFCN)\n",
        "\n",
        "Our first model is a Simple Fully Convolutional Network (FCN). This is a standard CNN architecture that preserves spatial dimensions from input to output, making it suitable for semantic segmentation. It consists of several blocks of `Convolution -> Batch Normalization -> ReLU` layers that extract features, followed by a final `1x1 convolution` that acts as a pixel-wise classifier."
      ],
      "metadata": {
        "id": "YtTTOOqmZD9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwYn4plXKgeX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JZ-Tm8_KkNs"
      },
      "outputs": [],
      "source": [
        "class simpleFCN(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "      \"\"\"\n",
        "        Args:\n",
        "            in_channels (int): Number of channels in the input image (4 for us: R,G,B,NIR).\n",
        "            num_classes (int): Number of output classes (3 for us: Non-Forest, Forest, Deforested).\n",
        "      \"\"\"\n",
        "      super(simpleFCN, self).__init__()\n",
        "\n",
        "      # --- The \"Encoder\" Path ---\n",
        "      # We create a series of blocks that extract features.\n",
        "      # The padding='same' argument is crucial ensures that the output  height and width are the same as the input height and width.\n",
        "\n",
        "      # Block 1: Input -> 16 channels\n",
        "      self.block1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, 16, kernel_size=3, padding='same'),\n",
        "          nn.BatchNorm2d(16),\n",
        "          nn.ReLU(inplace=True)\n",
        "      )\n",
        "\n",
        "      # Block 2: 16 -> 32 channels\n",
        "      self.block2 = nn.Sequential(\n",
        "          nn.Conv2d(16, 32, kernel_size=3, padding='same'),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.ReLU(inplace=True)\n",
        "      )\n",
        "\n",
        "      # Block 3: 32 -> 64 channels\n",
        "      self.block3 = nn.Sequential(\n",
        "          nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(inplace=True)\n",
        "      )\n",
        "\n",
        "      # The Final Classifier Layer\n",
        "      # This 1x1 convolution acts as a pixel-wise classifier.\n",
        "      # It takes the 64 feature channels and condenses them down to\n",
        "      # 3 output channels, one for each class score.\n",
        "      self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      \"\"\"The forward pass defines how data flows through the network.\"\"\"\n",
        "      # Pass input through the encoder blocks\n",
        "      x = self.block1(x)\n",
        "      x = self.block2(x)\n",
        "      x = self.block3(x)\n",
        "\n",
        "      # Pass through the final classifier\n",
        "      x = self.final_conv(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader,\n",
        "                scheduler, num_epochs, patience, checkpoint_path):\n",
        "    \"\"\"\n",
        "    A master function to handle the complete training and validation loop.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network model to train.\n",
        "        criterion (torch.nn.Module): The loss function.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        train_loader (DataLoader): DataLoader for the training set.\n",
        "        val_loader (DataLoader): DataLoader for the validation set.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler. Can be None.\n",
        "        num_epochs (int): The maximum number of epochs to train for.\n",
        "        patience (int): The patience for early stopping.\n",
        "        checkpoint_path (str): The file path to save/load checkpoints.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - model (torch.nn.Module): The model with the best weights loaded.\n",
        "            - train_loss_history (list): A list of average training losses per epoch.\n",
        "            - val_loss_history (list): A list of average validation losses per epoch.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Starting training on device: {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Checkpointing Logic\n",
        "    start_epoch, best_val_loss, epochs_no_improve = 0, float('inf'), 0\n",
        "    train_loss_history, val_loss_history = [], []\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Resuming training from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        train_loss_history = checkpoint['train_loss_history']\n",
        "        val_loss_history = checkpoint['val_loss_history']\n",
        "        epochs_no_improve = checkpoint['epochs_no_improve']\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.to(device)\n",
        "    else:\n",
        "        print(\"Starting new training run.\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\nEarly stopping condition already met at loaded checkpoint (Patience: {epochs_no_improve}/{patience}).\")\n",
        "        print(\"Skipping training loop.\")\n",
        "        return model, train_loss_history, val_loss_history\n",
        "\n",
        "    best_model_weights = model.state_dict().copy()\n",
        "\n",
        "    # The training loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [T]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "        avg_val_loss = running_val_loss / len(val_loader)\n",
        "        val_loss_history.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            old_lr = optimizer.param_groups[0]['lr']\n",
        "            scheduler.step(avg_val_loss)\n",
        "            new_lr = optimizer.param_groups[0]['lr']\n",
        "            if new_lr < old_lr:\n",
        "                print(f\"  -> Learning rate reduced from {old_lr} to {new_lr}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss, epochs_no_improve = avg_val_loss, 0\n",
        "            best_model_weights = model.state_dict().copy()\n",
        "            print(\"  -> Validation loss improved!\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  -> No improvement. Patience: {epochs_no_improve}/{patience}\")\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'train_loss_history': train_loss_history, 'val_loss_history': val_loss_history,\n",
        "            'epochs_no_improve': epochs_no_improve,\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"\\nEarly stopping triggered!\"); break\n",
        "\n",
        "    print(\"\\nTraining complete. Loading best model weights.\")\n",
        "    model.load_state_dict(best_model_weights)\n",
        "\n",
        "    return model, train_loss_history, val_loss_history"
      ],
      "metadata": {
        "id": "jswywue1pxSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This dictionary will be populated by the analysis cell of each experiment\n",
        "# We initialize it here to ensure it's created only once per session.\n",
        "all_results = {}"
      ],
      "metadata": {
        "id": "AdT-snXbwk-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UXszpwPQGOS"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm # progress bar\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IT0EYalUHAM"
      },
      "outputs": [],
      "source": [
        "# Instantiate our custom dataset\n",
        "dataset = ForestDataset(root_dir=PROJECT_FOLDER)\n",
        "print(f\"Found {len(dataset)} total samples.\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Instantiate the model\n",
        "model = simpleFCN(in_channels = 4, num_classes = 3)\n",
        "\n",
        "# Prepare datasets and dataloaders\n",
        "\n",
        "# Define the split ratio\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42) # fixed seed for reproducability\n",
        ")\n",
        "\n",
        "print(f\"  - Training samples:   {len(train_dataset)}\")\n",
        "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
        "print(f\"  - Test samples:       {len(test_dataset)}\")\n",
        "\n",
        "# Create dataset for both sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluating the Baseline Model\n",
        "\n",
        "We train the `SimpleFCN` using a robust training loop that includes a train/validation/test split, early stopping to prevent overfitting, and checkpointing to save progress. We evaluate the final model using a comprehensive set of metrics, including Overall Accuracy, Precision, Recall, F1-Score, and Intersection over Union (IoU), to get a complete picture of its performance."
      ],
      "metadata": {
        "id": "HwDfBeJkZOHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66-5xr4iQLKM"
      },
      "outputs": [],
      "source": [
        "# Baseline FCN (Unweighted Loss)\n",
        "print(\"Baseline FCN \")\n",
        "\n",
        "# Define all components\n",
        "baseline_fcn = simpleFCN(in_channels=4, num_classes=3)\n",
        "baseline_optimizer = optim.Adam(baseline_fcn.parameters(), lr=0.001)\n",
        "baseline_criterion = nn.CrossEntropyLoss() # Standard, unweighted loss\n",
        "baseline_chk_path = os.path.join(PROJECT_FOLDER, 'training_checkpoint.pth')\n",
        "baseline_final_path = os.path.join(PROJECT_FOLDER, 'best_baseline_fcn_model.pth')\n",
        "\n",
        "# Call the master training function\n",
        "baseline_fcn, baseline_train_hist, baseline_val_hist = train_model(\n",
        "    model=baseline_fcn,\n",
        "    criterion=baseline_criterion,\n",
        "    optimizer=baseline_optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    scheduler=None,\n",
        "    num_epochs=25,\n",
        "    patience=5,\n",
        "    checkpoint_path=baseline_chk_path\n",
        ")\n",
        "\n",
        "# Save the final best model\n",
        "print(f\"\\nSaving final model to {baseline_final_path}\")\n",
        "torch.save(baseline_fcn.state_dict(), baseline_final_path)\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(baseline_train_hist, label='Training Loss')\n",
        "plt.plot(baseline_val_hist, label='Validation Loss')\n",
        "plt.title('Baseline FCN: Training & Validation Loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgaG6yUI2VIZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "def evaluate_model(model, model_name, test_loader, device, all_results_dict):\n",
        "    \"\"\"\n",
        "    A master function to perform a complete evaluation (metrics and visualization)\n",
        "    on a trained model, print a detailed report, and store a curated summary.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Comprehensive Evaluation for: {model_name} ---\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=f\"Testing {model_name}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images); _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.append(predicted.cpu().numpy().flatten())\n",
        "            all_labels.append(labels.cpu().numpy().flatten())\n",
        "    all_preds, all_labels = np.concatenate(all_preds), np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])\n",
        "    overall_accuracy = 100 * np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
        "\n",
        "    class_names = [\"Non-Forest\", \"Forest\", \"Deforested\"]\n",
        "    precisions, recalls, f1_scores, ious = [], [], [], []\n",
        "\n",
        "    # Print Detailed Per-Class Report\n",
        "    print(\"\\n--- Detailed Per-Class Metrics ---\")\n",
        "    for i in range(len(class_names)):\n",
        "        TP = conf_matrix[i, i]; FP = np.sum(conf_matrix[:, i]) - TP; FN = np.sum(conf_matrix[i, :]) - TP\n",
        "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
        "        precisions.append(precision); recalls.append(recall); f1_scores.append(f1); ious.append(iou)\n",
        "\n",
        "        print(f\"\\nClass: {class_names[i]} (Class {i})\")\n",
        "        print(f\"  - Precision: {precision:.3f}\")\n",
        "        print(f\"  - Recall:    {recall:.3f}\")\n",
        "        print(f\"  - F1-Score:  {f1:.3f}\")\n",
        "        print(f\"  - IoU:       {iou:.3f}\")\n",
        "\n",
        "    # Print Summary Metrics\n",
        "    print(\"\\n\" + \"-\" * 30)\n",
        "    print(\"--- Summary Metrics ---\")\n",
        "    print(f\"Overall Pixel Accuracy: {overall_accuracy:.2f}%\")\n",
        "    print(f\"Macro-Averaged F1-Score: {np.mean(f1_scores):.3f}\")\n",
        "    print(f\"Mean IoU (mIoU):         {np.mean(ious):.3f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Store Curated Results for Final Comparison Table\n",
        "    all_results_dict[model_name] = {\n",
        "        \"Overall Accuracy\": f\"{overall_accuracy:.2f}%\", \"Mean IoU (mIoU)\": f\"{np.mean(ious):.3f}\",\n",
        "        \"Macro F1-Score\": f\"{np.mean(f1_scores):.3f}\", \"F1: Non-Forest\": f\"{f1_scores[0]:.3f}\",\n",
        "        \"F1: Forest\": f\"{f1_scores[1]:.3f}\", \"F1: Deforested\": f\"{f1_scores[2]:.3f}\",\n",
        "        \"Recall: Deforested\": f\"{recalls[2]:.3f}\", \"Precision: Deforested\": f\"{precisions[2]:.3f}\",\n",
        "    }\n",
        "    print(f\"\\nCurated results for '{model_name}' have been stored for the final comparison table.\")\n",
        "\n",
        "    return all_results_dict\n",
        "\n",
        "def visualize_predictions(dataset, model, device, model_name, num_samples=3):\n",
        "    \"\"\"A master function to visualize model predictions on random samples.\"\"\"\n",
        "    print(f\"\\n--- Starting Qualitative Visualization for: {model_name} ---\")\n",
        "    color_map = np.array([[0, 0, 0], [0, 0.8, 0], [1, 0, 0]], dtype=np.float32)\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n",
        "    fig.suptitle(f'Prediction Analysis for: {model_name}', fontsize=16)\n",
        "    model.eval()\n",
        "    for i in range(num_samples):\n",
        "        sample_idx = random.randint(0, len(dataset) - 1)\n",
        "        image, label = dataset[sample_idx]\n",
        "        rgb_image = image.numpy()[:3, :, :].transpose(1, 2, 0)\n",
        "        p2, p98 = np.percentile(rgb_image, (2, 98)); rgb_image_stretched = np.clip((rgb_image - p2) / (p98 - p2), 0, 1)\n",
        "        label_viz = color_map[label.numpy()]\n",
        "        with torch.no_grad():\n",
        "            output = model(image.unsqueeze(0).to(device)); _, predicted = torch.max(output, 1)\n",
        "        predicted_viz = color_map[predicted.cpu().squeeze().numpy()]\n",
        "        axes[i, 0].imshow(rgb_image_stretched); axes[i, 0].set_title(f'Input (Sample #{sample_idx})'); axes[i, 0].axis('off')\n",
        "        axes[i, 1].imshow(label_viz); axes[i, 1].set_title('Ground Truth'); axes[i, 1].axis('off')\n",
        "        axes[i, 2].imshow(predicted_viz); axes[i, 2].set_title('Model Prediction'); axes[i, 2].axis('off')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrqZjSvaYZ3K"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Call the evaluation function (it also stores the results)\n",
        "all_results = evaluate_model(\n",
        "    model=baseline_fcn,\n",
        "    model_name=\"Baseline FCN\",\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    all_results_dict=all_results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the visualization function\n",
        "visualize_predictions(\n",
        "    dataset=test_dataset,\n",
        "    model=baseline_fcn,\n",
        "    device=device,\n",
        "    model_name=\"Baseline FCN\"\n",
        ")"
      ],
      "metadata": {
        "id": "d6JEk_Hrc19x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qITa8fkd5un"
      },
      "source": [
        "## 3: Comparative Analysis and Model Improvement\n",
        "\n",
        "The results from our baseline model revealed a critical weakness: a complete failure to identify the rare \"Deforested\" class due to severe class imbalance. In this phase, we will implement and compare different strategies to solve this problem and improve overall performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR3cCuguekrG"
      },
      "source": [
        "### 3.1: Diagnosing and Addressing Class Imbalance\n",
        "\n",
        "Our first experiment is to tackle the class imbalance directly. We analyze our training dataset to calculate the precise frequency of each class. Based on this, we compute inverse frequency weights, which we will use in a `WeightedCrossEntropyLoss` function. This heavily penalizes mistakes made on the rare classes, forcing the model to pay attention to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF15TuKNagxg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define the path for our saved weights file\n",
        "WEIGHTS_FILE_PATH = os.path.join(PROJECT_FOLDER, 'class_weights.pth')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Check if weights file exists\n",
        "if os.path.exists(WEIGHTS_FILE_PATH):\n",
        "    # load it and skip the calculation\n",
        "    print(f\"Found existing weights file. Loading from: {WEIGHTS_FILE_PATH}\")\n",
        "    class_weights_tensor = torch.load(WEIGHTS_FILE_PATH, map_location=device)\n",
        "\n",
        "else:\n",
        "    # If it does not exist, perform the calculation and save the result\n",
        "    print(\"Weights file not found. Calculating class frequencies from the training dataset...\")\n",
        "\n",
        "    # We use the train_dataset to calculate weights with temporary DataLoader and a large batch size\n",
        "    weight_loader = DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"Calculating class frequencies from the training dataset...\")\n",
        "    # Initialize a dictionary to hold the pixel counts for each class\n",
        "    class_pixel_counts = {0: 0, 1: 0, 2: 0}\n",
        "\n",
        "    # Loop through the entire training set\n",
        "    for _, labels in tqdm(weight_loader, desc=\"Counting Pixels\"):\n",
        "        # For each batch of labels, count the occurrences of each class value\n",
        "        unique, counts = np.unique(labels.numpy(), return_counts=True)\n",
        "        for u, c in zip(unique, counts):\n",
        "            if u in class_pixel_counts:\n",
        "                class_pixel_counts[u] += c\n",
        "\n",
        "    print(\"\\n--- Class Distribution Analysis ---\")\n",
        "    total_pixels = sum(class_pixel_counts.values())\n",
        "    print(f\"Total pixels in training set: {total_pixels:,}\")\n",
        "\n",
        "    class_names = [\"0: Non-Forest\", \"1: Forest\", \"2: Deforested\"]\n",
        "    class_weights = []\n",
        "\n",
        "    for i in range(len(class_names)):\n",
        "        frequency = class_pixel_counts[i] / total_pixels\n",
        "        # Calculate weight as inverse of frequency\n",
        "        weight = 1.0 / (frequency + 1e-6) # Add epsilon to avoid division by zero\n",
        "        class_weights.append(weight)\n",
        "\n",
        "        print(f\"  - {class_names[i]}: {class_pixel_counts[i]:,} pixels ({frequency:.4%})\")\n",
        "\n",
        "    # Normalize the weights so they aren't astronomically large\n",
        "    class_weights = np.array(class_weights)\n",
        "    class_weights = class_weights / np.sum(class_weights)\n",
        "\n",
        "    print(\"\\n--- Final Calculated Weights ---\")\n",
        "    for i in range(len(class_names)):\n",
        "        print(f\"  - {class_names[i]}: {class_weights[i]:.4f}\")\n",
        "\n",
        "    # Convert weights to a PyTorch tensor to be used in the loss function\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "    print(\"\\nClass weights tensor created and ready for use in the loss function.\")\n",
        "\n",
        "    # save the weight tensor to class_weights file\n",
        "    WEIGHTS_FILE_PATH = os.path.join(PROJECT_FOLDER, 'class_weights.pth')\n",
        "\n",
        "    print(f\"\\nSaving class weights to: {WEIGHTS_FILE_PATH}\")\n",
        "    torch.save(class_weights_tensor, WEIGHTS_FILE_PATH)\n",
        "    print(\"Weights saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2: Re-training the FCN with Weighted Loss\n",
        "\n",
        "To isolate the effect of our intervention, we re-train the exact same `SimpleFCN` architecture from scratch, but this time using our new weighted loss function. This allows us to scientifically measure the impact of this single change on model performance, particularly on the \"Deforested\" class."
      ],
      "metadata": {
        "id": "VDvKFTnHZ6ee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kydm1Th5kztO"
      },
      "outputs": [],
      "source": [
        "# FCN with Weighted Loss ---\n",
        "print(\"--- Starting Experiment 2: Weighted FCN ---\")\n",
        "\n",
        "weighted_fcn = simpleFCN(in_channels=4, num_classes=3)\n",
        "weighted_fcn_optimizer = optim.Adam(weighted_fcn.parameters(), lr=0.001)\n",
        "\n",
        "# weighted criterion\n",
        "weighted_fcn_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "weighted_fcn_chk_path = os.path.join(PROJECT_FOLDER, 'weighted_fcn_checkpoint.pth')\n",
        "weighted_fcn_final_path = os.path.join(PROJECT_FOLDER, 'best_weighted_fcn_model.pth')\n",
        "\n",
        "# call the master training function\n",
        "weighted_fcn, weighted_fcn_train_hist, weighted_fcn_val_hist = train_model(\n",
        "    model=weighted_fcn,\n",
        "    criterion=weighted_fcn_criterion,\n",
        "    optimizer=weighted_fcn_optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    scheduler=None, # No scheduler for this run\n",
        "    num_epochs=25,\n",
        "    patience=5,\n",
        "    checkpoint_path=weighted_fcn_chk_path\n",
        ")\n",
        "\n",
        "# save the final best model\n",
        "print(f\"\\nSaving final model to {weighted_fcn_final_path}\")\n",
        "torch.save(weighted_fcn.state_dict(), weighted_fcn_final_path)\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# plot the results for this experiment\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(weighted_fcn_train_hist, label='Training Loss')\n",
        "plt.plot(weighted_fcn_val_hist, label='Validation Loss')\n",
        "plt.title('Weighted FCN: Training & Validation Loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wcpnGnOw1Ba"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "model_name_exp2 = \"Weighted FCN\"\n",
        "model_path_exp2 = os.path.join(PROJECT_FOLDER, 'best_weighted_fcn_model.pth')\n",
        "\n",
        "# Load the trained model\n",
        "model_exp2 = simpleFCN(in_channels=4, num_classes=3)\n",
        "model_exp2.load_state_dict(torch.load(model_path_exp2, map_location=device))\n",
        "model_exp2.to(device)\n",
        "\n",
        "# Call the master evaluation function (it prints, stores results)\n",
        "all_results = evaluate_model(\n",
        "    model=model_exp2,\n",
        "    model_name=model_name_exp2,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    all_results_dict=all_results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6TeZd6W5BeN"
      },
      "outputs": [],
      "source": [
        "# Call the master visualization function\n",
        "visualize_predictions(\n",
        "    dataset=test_dataset,\n",
        "    model=model_exp2,\n",
        "    device=device,\n",
        "    model_name=model_name_exp2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3: Designing an Advanced Architecture (U-Net)\n",
        "\n",
        "Our analysis showed that while weighted loss helped the FCN *detect* the rare class, its predictions were spatially noisy. To address this, we now implement a more advanced architecture: the **U-Net**. The U-Net's encoder-decoder structure with skip connections is the industry standard for semantic segmentation and is specifically designed to produce clean, spatially precise prediction maps."
      ],
      "metadata": {
        "id": "g6n8Xvtmf2bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOp1WwdW5q9Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"A helper module with two convolutions: (conv => BN => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"The U-Net architecture for semantic segmentation.\"\"\"\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # --- Encoder (Downsampling Path) ---\n",
        "        self.inc = DoubleConv(in_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
        "\n",
        "        # --- Decoder (Upsampling Path) ---\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv1 = DoubleConv(512, 256) # 256 from upsample + 256 from skip\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv2 = DoubleConv(256, 128) # 128 from upsample + 128 from skip\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv3 = DoubleConv(128, 64) # 64 from upsample + 64 from skip\n",
        "\n",
        "        # --- Final Classifier Layer ---\n",
        "        self.outc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)     # -> 64x64x64\n",
        "        x2 = self.down1(x1)  # -> 32x32x128\n",
        "        x3 = self.down2(x2)  # -> 16x16x256\n",
        "        x4 = self.down3(x3)  # -> 8x8x512\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x4)      # Upsample to 16x16x256\n",
        "        # Concatenate with skip connection from encoder\n",
        "        x = torch.cat([x3, x], dim=1) # -> 16x16x512\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.up2(x)      # Upsample to 32x32x128\n",
        "        x = torch.cat([x2, x], dim=1) # -> 32x32x256\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.up3(x)      # Upsample to 64x64x64\n",
        "        x = torch.cat([x1, x], dim=1) # -> 64x64x128\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Final output\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "unet_model = UNet(in_channels=4, num_classes=3)\n",
        "print(\"U-Net model instantiated successfully.\")\n",
        "\n",
        "# Create a dummy input tensor with the correct shape\n",
        "dummy_batch = torch.randn(4, 4, 64, 64) # (batch_size, channels, height, width)\n",
        "\n",
        "# Pass the dummy data through the model\n",
        "output = unet_model(dummy_batch)\n",
        "\n",
        "# Print the output shape to confirm it's correct\n",
        "print(\"\\n--- Testing with a dummy batch ---\")\n",
        "print(f\"Shape of the input batch:  {dummy_batch.shape}\")\n",
        "print(f\"Shape of the output batch: {output.shape}\")\n",
        "\n",
        "# Check if the output shape matches the input spatial dimensions\n",
        "if output.shape == (4, 3, 64, 64):\n",
        "    print(\"\\nSUCCESS! The U-Net output shape is correct.\")\n",
        "else:\n",
        "    print(f\"\\nERROR! Unexpected output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8-2HadV6WP9"
      },
      "outputs": [],
      "source": [
        "# U-Net with Weighted Loss (No Scheduler)\n",
        "print(\"--- Starting Experiment 3: U-Net with Weighted Loss ---\")\n",
        "\n",
        "# 1. Define components for this experiment\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Use the 'unet_model' variable from the previous cell where the architecture was defined\n",
        "unet_optimizer = optim.Adam(unet_model.parameters(), lr=0.001)\n",
        "unet_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Define unique paths for this experiment's artifacts\n",
        "unet_chk_path = os.path.join(PROJECT_FOLDER, 'unet_checkpoint.pth')\n",
        "unet_final_path = os.path.join(PROJECT_FOLDER, 'best_unet_model.pth')\n",
        "\n",
        "# 2. Call the master training function\n",
        "unet_model, unet_train_hist, unet_val_hist = train_model(\n",
        "    model=unet_model,\n",
        "    criterion=unet_criterion,\n",
        "    optimizer=unet_optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    scheduler=None, # <-- No scheduler for this run\n",
        "    num_epochs=25,\n",
        "    patience=5,\n",
        "    checkpoint_path=unet_chk_path\n",
        ")\n",
        "\n",
        "# 3. Save the final best model from this run\n",
        "print(f\"\\nSaving final model to {unet_final_path}\")\n",
        "torch.save(unet_model.state_dict(), unet_final_path)\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# 4. Plot the results for this specific experiment\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(unet_train_hist, label='Training Loss')\n",
        "plt.plot(unet_val_hist, label='Validation Loss')\n",
        "plt.title('U-Net: Training & Validation Loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv9jmbeRtO36"
      },
      "outputs": [],
      "source": [
        "# U-Net with Weighted Loss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "model_name_exp3 = \"U-Net\"\n",
        "model_path_exp3 = os.path.join(PROJECT_FOLDER, 'best_unet_model.pth')\n",
        "\n",
        "# Load the trained model\n",
        "eval_unet_model = UNet(in_channels=4, num_classes=3)\n",
        "eval_unet_model.load_state_dict(torch.load(model_path_exp3, map_location=device))\n",
        "eval_unet_model.to(device)\n",
        "\n",
        "# Call the master evaluation function\n",
        "all_results = evaluate_model(\n",
        "    model=eval_unet_model,\n",
        "    model_name=model_name_exp3,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    all_results_dict=all_results\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the master visualization function\n",
        "visualize_predictions(\n",
        "    dataset=test_dataset,\n",
        "    model=eval_unet_model,\n",
        "    device=device,\n",
        "    model_name=model_name_exp3\n",
        "    )\n"
      ],
      "metadata": {
        "id": "H7hmhVEmsyaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4: Training the U-Net with scheduler\n",
        "\n",
        "The U-Net is a much more powerful and complex model, making it prone to unstable training. To manage this, we train it using our proven weighted loss function and also implement a `ReduceLROnPlateau` learning rate scheduler. This scheduler automatically reduces the learning rate when the validation loss plateaus, allowing for a more stable and effective convergence to a good solution."
      ],
      "metadata": {
        "id": "vrBKZOCzhjFT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UxpZom0WABn"
      },
      "outputs": [],
      "source": [
        "# U-Net with Weighted Loss and LR Scheduler\n",
        "print(\"--- Starting Experiment 4: Stable U-Net ---\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Instantiate a fresh U-Net model for this experiment\n",
        "stable_unet_model = UNet(in_channels=4, num_classes=3)\n",
        "stable_unet_optimizer = optim.Adam(stable_unet_model.parameters(), lr=0.001)\n",
        "stable_unet_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# For this experiment, we define the scheduler\n",
        "stable_unet_scheduler = ReduceLROnPlateau(stable_unet_optimizer, mode='min', factor=0.1, patience=2)\n",
        "\n",
        "# Define unique paths for this experiment's artifacts\n",
        "stable_unet_chk_path = os.path.join(PROJECT_FOLDER, 'unet_scheduled_checkpoint.pth')\n",
        "stable_unet_final_path = os.path.join(PROJECT_FOLDER, 'best_unet_scheduled_model.pth')\n",
        "\n",
        "# Call the master training function\n",
        "stable_unet_model, stable_unet_train_hist, stable_unet_val_hist = train_model(\n",
        "    model=stable_unet_model,\n",
        "    criterion=stable_unet_criterion,\n",
        "    optimizer=stable_unet_optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    scheduler=stable_unet_scheduler, # <-- Pass the scheduler here\n",
        "    num_epochs=25,\n",
        "    patience=5,\n",
        "    checkpoint_path=stable_unet_chk_path\n",
        ")\n",
        "\n",
        "# Save the final best model from this run\n",
        "print(f\"\\nSaving final model to {stable_unet_final_path}\")\n",
        "torch.save(stable_unet_model.state_dict(), stable_unet_final_path)\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# Plot the results for this specific experiment\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(stable_unet_train_hist, label='Training Loss')\n",
        "plt.plot(stable_unet_val_hist, label='Validation Loss')\n",
        "plt.title('Stable U-Net with LR Scheduler: Training & Validation Loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJTzmxkHWH6S"
      },
      "outputs": [],
      "source": [
        "# defining necessary components\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "model_name_exp4 = \"Stable U-Net\"\n",
        "model_path_exp4 = os.path.join(PROJECT_FOLDER, 'best_unet_scheduled_model.pth')\n",
        "\n",
        "# Load the trained model\n",
        "eval_stable_unet_model = UNet(in_channels=4, num_classes=3)\n",
        "eval_stable_unet_model.load_state_dict(torch.load(model_path_exp4, map_location=device))\n",
        "eval_stable_unet_model.to(device)\n",
        "\n",
        "#  Call the master evaluation function\n",
        "all_results = evaluate_model(\n",
        "    model=eval_stable_unet_model,\n",
        "    model_name=model_name_exp4,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    all_results_dict=all_results\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNS31MGNWIzX"
      },
      "outputs": [],
      "source": [
        "# Call the master visualization function\n",
        "visualize_predictions(\n",
        "    dataset=test_dataset,\n",
        "    model=eval_stable_unet_model,\n",
        "    device=device,\n",
        "    model_name=model_name_exp4\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BT3sl1Gu2DRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Results and Conclusion\n",
        "\n",
        "After conducting our three main experiments, we can now compare the results side-by-side to draw our final conclusions and declare the most effective approach for this task.\n",
        "\n",
        "### Head-to-Head Model Comparison\n",
        "\n",
        " The following table summarizes the performance of our three models on the untouched test set. The metrics clearly show a story of methodical improvement and trade-offs. The **Stable U-Net** emerges as the best overall model, demonstrating a strong balance between performance on all classes."
      ],
      "metadata": {
        "id": "AKdnbjRBiXQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "print(\"--- Generating Final Head-to-Head Model Comparison ---\")\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Define the desired order of rows and columns for the final report\n",
        "    metric_order = [\"Overall Accuracy\", \"Mean IoU (mIoU)\", \"Macro F1-Score\", \"F1: Non-Forest\",\n",
        "                    \"F1: Forest\", \"F1: Deforested\", \"Recall: Deforested\", \"Precision: Deforested\"]\n",
        "    column_order = [\"Baseline FCN\", \"Weighted FCN\", \"U-Net\", \"Stable U-Net\"]\n",
        "\n",
        "    # Ensure only existing columns are used to prevent errors\n",
        "    final_column_order = [col for col in column_order if col in results_df.columns]\n",
        "\n",
        "    results_df = results_df.reindex(metric_order)[final_column_order]\n",
        "\n",
        "    display(Markdown(results_df.to_markdown()))\n",
        "else:\n",
        "    print(\"The 'all_results' dictionary is empty. Please run the 'Analyze Experiment' cells first.\")"
      ],
      "metadata": {
        "id": "hOzj7Pq_0CiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Analysis: Side-by-Side Model Predictions\n",
        "\n",
        "The quantitative metrics in the table above show the *what*, but this visual analysis reveals the *why*. By feeding the exact same input image to all four of our trained models, we can directly compare their behavior and understand their different strengths and weaknesses.\n",
        "\n",
        "**What to look for in the plots below:**\n",
        "*   **Baseline FCN:** Notice its inability to predict \"Deforested\" (red).\n",
        "*   **Weighted FCN:** See how it begins to predict \"Deforested\" (high recall), but often incorrectly and with a noisy, speckled appearance (low precision).\n",
        "*   **Unstable U-Net:** Observe the volatile and often nonsensical predictions, confirming its failed training.\n",
        "*   **Stable U-Net (Winner):** Look for the much cleaner, more spatially coherent shapes and its more reasoned (though still imperfect) attempts at predicting all three classes."
      ],
      "metadata": {
        "id": "RPe13UOwiboD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "\n",
        "def visualize_all_models_comparison(dataset, models_dict, device, num_samples=3):\n",
        "    \"\"\"\n",
        "    Visualizes predictions from multiple models side-by-side for the same input images.\n",
        "    \"\"\"\n",
        "    color_map = np.array([[0, 0, 0], [0, 0.8, 0], [1, 0, 0]], dtype=np.float32) # Black, Green, Red\n",
        "\n",
        "    # +2 columns for the i/p and ground truth\n",
        "    num_cols = len(models_dict) + 2\n",
        "    fig, axes = plt.subplots(num_samples, num_cols, figsize=(num_cols * 4, num_samples * 4.5))\n",
        "    fig.suptitle('Side-by-Side Model Prediction Comparison', fontsize=20)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # picking random sample\n",
        "        sample_idx = random.randint(0, len(dataset) - 1)\n",
        "        image, label = dataset[sample_idx]\n",
        "\n",
        "        rgb_image = image.numpy()[:3, :, :].transpose(1, 2, 0)\n",
        "        p2, p98 = np.percentile(rgb_image, (2, 98)); rgb_image_stretched = np.clip((rgb_image - p2) / (p98 - p2), 0, 1)\n",
        "        label_viz = color_map[label.numpy()]\n",
        "\n",
        "        # i/p image\n",
        "        ax = axes[i, 0]\n",
        "        ax.imshow(rgb_image_stretched); ax.set_title(f'Input (Sample #{sample_idx})'); ax.axis('off')\n",
        "\n",
        "        # ground truth\n",
        "        ax = axes[i, 1]\n",
        "        ax.imshow(label_viz); ax.set_title('Ground Truth'); ax.axis('off')\n",
        "\n",
        "        # trained models prediction\n",
        "        for j, (model_name, model) in enumerate(models_dict.items()):\n",
        "            ax = axes[i, j + 2] # Start plotting in the third column\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                output = model(image.unsqueeze(0).to(device)); _, predicted = torch.max(output, 1)\n",
        "            predicted_viz = color_map[predicted.cpu().squeeze().numpy()]\n",
        "            ax.imshow(predicted_viz); ax.set_title(model_name); ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# load models for comparision\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "models_to_load = {\n",
        "    \"Baseline FCN\": {\n",
        "        \"path\": os.path.join(PROJECT_FOLDER, 'best_baseline_fcn_model.pth'),\n",
        "        \"architecture\": simpleFCN(in_channels=4, num_classes=3)\n",
        "    },\n",
        "    \"Weighted FCN\": {\n",
        "        \"path\": os.path.join(PROJECT_FOLDER, 'best_weighted_fcn_model.pth'),\n",
        "        \"architecture\": simpleFCN(in_channels=4, num_classes=3)\n",
        "    },\n",
        "    \"Unstable U-Net\": {\n",
        "        \"path\": os.path.join(PROJECT_FOLDER, 'best_unet_model.pth'), # <-- Corrected Path\n",
        "        \"architecture\": UNet(in_channels=4, num_classes=3)\n",
        "    },\n",
        "    \"Stable U-Net\": {\n",
        "        \"path\": os.path.join(PROJECT_FOLDER, 'best_unet_scheduled_model.pth'), # <-- Corrected Path\n",
        "        \"architecture\": UNet(in_channels=4, num_classes=3)\n",
        "    }\n",
        "}\n",
        "loaded_models = {}\n",
        "\n",
        "print(\"Loading all trained models for final comparison...\")\n",
        "for name, model_info in models_to_load.items():\n",
        "    path = model_info['path']\n",
        "    model_instance = model_info['architecture']\n",
        "    if os.path.exists(path):\n",
        "        model_instance.load_state_dict(torch.load(path, map_location=device))\n",
        "        model_instance.to(device)\n",
        "        loaded_models[name] = model_instance\n",
        "    else:\n",
        "        print(f\"Warning: Model file not found for '{name}' at '{path}'. It will be excluded from visualization.\")\n",
        "\n",
        "if loaded_models:\n",
        "    print(\"\\nVisualizing predictions on random samples from the TEST set...\")\n",
        "    visualize_all_models_comparison(test_dataset, loaded_models, device, num_samples=3)\n",
        "else:\n",
        "    print(\"No models were loaded. Cannot run visualization.\")"
      ],
      "metadata": {
        "id": "Bn0tfBStjakN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Conclusion & Future Work\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This project successfully developed and rigorously evaluated an end-to-end pipeline for semantic segmentation of deforestation in Nepal. Our methodical, comparative analysis of four distinct experimental setups yielded clear and actionable insights:\n",
        "\n",
        "1.  **Class Imbalance is the Dominant Challenge:** A baseline `SimpleFCN` model achieved a misleadingly high **Overall Accuracy of 86.82%** by completely ignoring the minority \"Deforested\" class, resulting in an **F1-Score of 0.0**.\n",
        "\n",
        "2.  **Weighted Loss is an Effective Countermeasure:** By introducing a `WeightedCrossEntropyLoss`, we successfully forced the model to learn the minority class, boosting the **Recall for \"Deforested\" from 0% to 58%**. This demonstrated a direct solution to the primary problem, albeit at the cost of precision.\n",
        "\n",
        "3.  **Advanced Architectures Improve Spatial Cohesion:** The `U-Net` architecture produced visually superior, more spatially coherent prediction maps compared to the noisy FCN outputs. However, its increased complexity led to training instability with a high learning rate.\n",
        "\n",
        "4.  **Stabilization is Key to Performance:** The winning model was the **`Stable U-Net`**, which combined the advanced U-Net architecture with the weighted loss function and a `ReduceLROnPlateau` learning rate scheduler. This model achieved the best balance of performance across all classes, with a final **Mean IoU of 0.465**. The key finding was that addressing class imbalance via weighted loss was essential, boosting the Recall for the 'Deforested' class from 0% to over 40%.\n",
        "\n",
        "The key remaining challenge is the low **precision** on the \"Deforested\" class, where the winning model still confuses naturally bright surfaces like riverbeds with cleared land, as evidenced by the qualitative visualizations.\n",
        "\n",
        "### Future Work\n",
        "\n",
        "To further improve performance, future experiments could include:\n",
        "\n",
        "*   **Advanced Architectures (e.g., Vision Transformer):** The primary weakness of our best model is a lack of global context. A Transformer-based segmentation architecture, such as **SegFormer**, could be implemented. Its self-attention mechanism is ideally suited to learn the long-range spatial relationships needed to distinguish a long, winding river from a small, isolated deforestation patch, directly targeting the precision problem.\n",
        "*   **Data Augmentation:** Applying random geometric and color augmentations to the training data to improve model robustness and generalization.\n",
        "*   **Post-Processing:** Using techniques like Conditional Random Fields (CRFs) to clean up and refine the final prediction maps based on the spatial relationships of the predictions themselves."
      ],
      "metadata": {
        "id": "J9BwyWDbi7dk"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}