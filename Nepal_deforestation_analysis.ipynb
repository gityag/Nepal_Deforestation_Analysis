{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zxWV0APoBaa"
   },
   "source": [
    "# Nepal Deforestation Analysis â€” Notebook\n",
    "\n",
    "**Author:** `Prayag Man Mane`  \n",
    "**Date:** `2025-09`  \n",
    "\n",
    "**Purpose:**  \n",
    "Reproducible pipeline to:  \n",
    "1. Quantify forest change in Nepal.  \n",
    "2. Build and compare segmentation models to detect deforestation.  \n",
    "3. Generate maps and results.\n",
    "\n",
    "**How to run:**  \n",
    "- Works in Google Colab or local Jupyter.  \n",
    "- Run cells in order, starting below.  \n",
    "- Authenticate Google Earth Engine and Google Drive when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVQcbaP7znzJ"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "This section ensures the notebook runs both on **Google Colab** and **local Jupyter notebooks**.\n",
    "\n",
    "We will:  \n",
    "1. Detect environment (Colab or local).  \n",
    "2. Install dependencies (if running in Colab).  \n",
    "3. Import libraries and check versions.  \n",
    "4. Create project directories (data, figures, models).  \n",
    "5. Set random seeds for reproducibility.  \n",
    "6. Authenticate and initialize Google Earth Engine (with project ID).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_MkawK2z0To"
   },
   "source": [
    "### 1.1: Environment Detection\n",
    "\n",
    "- Detect whether we are running inside **Google Colab** or **local Jupyter**.  \n",
    "- Mount Google Drive if in Colab (to persist files).  \n",
    "- Define a root project directory accordingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-4rbo74nu9v",
    "outputId": "1fe93c15-5442-42f2-d72d-96e24ec87007"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(\"Running on:\", \"Colab\" if IN_COLAB else \"Local Jupyter\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    PROJECT_ROOT = '/content/drive/MyDrive/NEPAL_DEFORESTATION'\n",
    "else:\n",
    "    PROJECT_ROOT = './nepal_deforestation'\n",
    "\n",
    "print(\"Project root directory:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56ljKrZiz6Xp"
   },
   "source": [
    "### 1.2: Install Dependencies (Colab only)\n",
    "\n",
    "- Install required packages if using Colab.  \n",
    "- Skip this step for Jupyter/local where packages should already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Zt4b9T7oLlM"
   },
   "outputs": [],
   "source": [
    "# --- Install required packages (Colab only) ---\n",
    "if IN_COLAB:\n",
    "    !pip install -q earthengine-api geemap rasterio torch torchvision scikit-learn matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCroTkO6z_Zj"
   },
   "source": [
    "\n",
    "### 1.3 Import Libraries & Check Versions\n",
    "\n",
    "- Import core libraries: `numpy`, `pandas`, `matplotlib`, etc.  \n",
    "- Import geospatial libraries: `earthengine-api`, `geemap`, `rasterio`.  \n",
    "- Import ML libraries: `torch`, `scikit-learn`.  \n",
    "- Print version info for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "MPQGb_dHoh3Q",
    "outputId": "1fdc3712-036c-46cd-e5b0-cd434d2e4610"
   },
   "outputs": [],
   "source": [
    "# --- Imports & version check ---\n",
    "import os, platform, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import torch\n",
    "import sklearn\n",
    "import ee, geemap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"rasterio:\", rasterio.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"geemap:\", geemap.__version__)\n",
    "print(\"earthengine-api:\", ee.__version__)\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJjQNm8C0DV6"
   },
   "source": [
    "### 1.4: Project Structure\n",
    "\n",
    "We create consistent directories for:  \n",
    "- `data/` â†’ raw and processed datasets.  \n",
    "- `figures/` â†’ visualizations and plots.  \n",
    "- `models/` â†’ saved ML models.  \n",
    "\n",
    "This ensures reproducibility and portability across environments.\n",
    "\n",
    "We fix random seeds across `numpy`, `torch`, and Pythonâ€™s `random` module.  \n",
    "If CUDA is available, GPU seeds are also fixed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "QdUBzqT9olIl",
    "outputId": "7fadc135-2d6b-428c-fc25-fbf1a7d08940"
   },
   "outputs": [],
   "source": [
    "# --- Project folders & reproducibility ---\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path(PROJECT_ROOT)\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "FIG_DIR  = PROJECT_DIR / \"figures\"\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "for d in [DATA_DIR, FIG_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Folders ready:\", [str(d) for d in [DATA_DIR, FIG_DIR, MODEL_DIR]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE7Buk9op4zB"
   },
   "source": [
    "### 1.5: Google Earth Engine Authentication & Project Setup\n",
    "\n",
    "To run Earth Engine exports, you **must provide a GCP project ID** that has billing enabled.\n",
    "\n",
    "- **In Google Colab**  \n",
    "  1. Go to **\"Secrets\"** (ðŸ”‘  in left sidebar).  \n",
    "  2. Add a new secret with key: `GCP_PROJECT_ID` and value: `<your-project-id>`.  \n",
    "  3. This notebook will automatically read it.\n",
    "\n",
    "- **In Jupyter / Local**  \n",
    "  1. Create a `.env` file in your project root.  \n",
    "  2. Add the line:  \n",
    "     ```\n",
    "     GCP_PROJECT_ID=your-project-id\n",
    "     ```\n",
    "  3. The notebook will load it using `python-dotenv`.\n",
    "\n",
    "If no project ID is provided, Earth Engine may still run, but **exports will fail** if billing is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "322iN0OqoqC5",
    "outputId": "f3545af0-217b-458b-cb46-04221236ee02"
   },
   "outputs": [],
   "source": [
    "# --- Authenticate & initialize Earth Engine with project_id ---\n",
    "\n",
    "import os, sys\n",
    "import ee\n",
    "\n",
    "project_id = None\n",
    "\n",
    "# Case 1: Running in Colab â†’ use colab userdata secrets\n",
    "if 'google.colab' in sys.modules:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        project_id = userdata.get(\"GCP_PROJECT_ID\")\n",
    "        print(\"Loaded project_id from Colab secrets.\")\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ Could not load from Colab secrets. Set it manually using:\")\n",
    "        print(\"  from google.colab import userdata; userdata.set('GCP_PROJECT_ID','your-project-id')\")\n",
    "\n",
    "# Case 2: Running locally â†’ load from .env file\n",
    "else:\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        project_id = os.getenv(\"GCP_PROJECT_ID\")\n",
    "        if project_id:\n",
    "            print(\"Loaded project_id from .env file.\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ Install python-dotenv with `pip install python-dotenv` to load from .env.\")\n",
    "\n",
    "# Final fallback\n",
    "if not project_id:\n",
    "    print(\"âš ï¸ No GCP_PROJECT_ID provided. You may not be able to export.\")\n",
    "\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(project=project_id)\n",
    "    print(\"âœ… Earth Engine initialized with project:\", project_id)\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project=project_id)\n",
    "    print(\"âœ… Earth Engine authenticated & initialized with project:\", project_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSpMqWdrq_lo"
   },
   "source": [
    "## 2. Analyze Forest Dataset in Nepal (2000-2024)\n",
    "\n",
    "1. Load the Nepal boundary (AOI) and Hansen dataset.\n",
    "2. Clip forest cover, loss, and gain layers to Nepal.\n",
    "3. Visualize tree cover, forest loss, and gain on an interactive map.\n",
    "4. Compute forest area statistics for 2000 and total loss/gain.\n",
    "5. Analyze the FarWest region separately, including maps and summary tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vn90cR34JSg"
   },
   "source": [
    "### 2.1: Nepal-wide Forest Analysis\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- Define Nepal AOI using the FAO GAUL Level 0 dataset.  \n",
    "- Load Hansen Global Forest Change dataset (2024 v1.12).  \n",
    "- Clip tree cover, loss, and gain layers to Nepal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "14QI_UwRrBsf",
    "outputId": "4e844ae0-65cd-478e-b0ab-5150fe121010"
   },
   "outputs": [],
   "source": [
    "# --- Load Nepal boundary (AOI) ---\n",
    "# Source: FAO GAUL (Global Administrative Units) Level 0 dataset on GEE\n",
    "nepal_aoi = ee.FeatureCollection(\"FAO/GAUL/2015/level0\") \\\n",
    "                .filter(ee.Filter.eq('ADM0_NAME', 'Nepal'))\n",
    "\n",
    "# Quick info\n",
    "print(\"Nepal AOI loaded.\")\n",
    "\n",
    "# --- Load Hansen Global Forest Change ---\n",
    "hansen = ee.Image(\"UMD/hansen/global_forest_change_2024_v1_12\")\n",
    "\n",
    "treecover2000 = hansen.select([\"treecover2000\"])\n",
    "\n",
    "loss = hansen.select(\"loss\")\n",
    "# gain = hansen.select(\"gain\")\n",
    "\n",
    "lossyear = hansen.select(\"lossyear\")\n",
    "\n",
    "# Clip to AOI\n",
    "treecover2000 = treecover2000.clip(nepal_aoi)\n",
    "loss = loss.clip(nepal_aoi)\n",
    "lossyear = lossyear.clip(nepal_aoi)\n",
    "\n",
    "print(\"Hansen dataset clipped to Nepal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgEnFuK94ZX8"
   },
   "source": [
    "### 2.2: â€” Hansen Global Forest Change Dataset & Visualization\n",
    "\n",
    "- Load `UMD/hansen/global_forest_change_2024_v1_12`.  \n",
    "- Extract bands: `treecover2000`, `loss`, `gain`, `lossyear`.  \n",
    "- Clip each band to Nepal AOI.  \n",
    "- Define visualization parameters:\n",
    "  - Treecover â†’ gradient palette (light â†’ dark green/blue).  \n",
    "  - Loss â†’ red.  \n",
    "- Display tree cover (2000), forest loss, and gain layers.  \n",
    "- Add AOI boundary for reference.  \n",
    "- `.selfMask()` is used for cleaner overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621,
     "referenced_widgets": [
      "2ed894e84b4644ef854669be6ad1f181",
      "c00f83e9076a4ae4a654b27d29dfe829",
      "eee6d37acf37422a98784862dc2a5ca2",
      "8a4ca47999374eaabfda117e504aa510",
      "3d7525bf7f184041a3a920c4cc126988",
      "8e1bc4ad8f724b6998f46f2b962df160",
      "d8e5c53bcb224175a92847e0e0ede64c",
      "44f6348554ab4fb8b5c6e0a5a7993cb5",
      "987408f27e1748e8b938e054a268ca0b",
      "df8e39f749f64504b261f22747213c02",
      "bae09b0ff8384e9598e96e326cb93a34",
      "c8bf7aa393bc46398747ea46c1d57347",
      "2d1c0b4f03e34becb0dd341ad3c5e528",
      "dd7f791bf270453b8e8c7be1587a441f",
      "54e331489eff486c895737024bc72119",
      "ebb2f9dd65574939bc8c787b13ee9246",
      "c8cdba2cd70e450793ce95f89063b141",
      "efc83c8d37374afd9be552cb964075ea",
      "e69f81f7797041b1a77367c889fb868e",
      "24890c35defd4fe8b34def118b382ebd",
      "dbf8a18ab5124f64a9decc803da1528a",
      "f3330bca46b6429e93bfe9556a26fcbe",
      "4e33ba52643e4ca1a2ad3537b9ddcfa4",
      "03665ce61e6b493bb5894528663792da",
      "d335907c57fb454c9b18b4802bec68d6",
      "16f3e310eb274f38b4a39d91ac0f07a0",
      "6d791a59d7c34b90b9e138ce056abf4b",
      "f56eb30332a14a2daf1d65fbdecd1753",
      "7312448123744a3d8353066c3668685c",
      "3cd270e995c84fc7aebac2c12c79f861",
      "8d49c23bf22d4db5a51f29bf8c5ad22b",
      "8d49df54f90d454e9b7beaf495566611",
      "769c14576138432a9da3e157543675bc",
      "b911d95e9eb04e1f82db8060352c438c",
      "fe6fc1739b3b41e7bd8a95859086afc3",
      "5ea675faf371414b8e9f7987fb2d2ed2",
      "d8a1e1d1d0224aaba3904e25826a0467",
      "454afb56198146ffb392136143f4d8e8",
      "589c30ec34654302981ad91fb3177b52",
      "509d34279d0d447a8e4ba4883aab2d12",
      "0e8c1cfa0c1048e998990958b6c7dd05",
      "dd3b0449298e4fc4ad199b88d910aec0",
      "fbc0d1ba2aa24f2ba5ce60fff7f0b937",
      "cddfae02ea1f4a828957fe3586654937",
      "c510d0edf2754cffbd880adb47871194",
      "476637a454ef4ce48d57b9c2eee2d8e3",
      "36f4888511394b148f5551727b52b5a0",
      "cc16e1eeeada4dda9eea503e9faae128",
      "c74ea9315a2c44f9ae10f89852e56f07",
      "6d52fdec99c545558981d851bd0aa49f",
      "4b39bb22a0e047eab0696e8a98731f54",
      "2d5cc2bcca9f41b68e5b127b529d73d1",
      "ac3cd86332144186bf98e6387cf58571",
      "03eabf712bda43f7ac94157b3a2ae576",
      "32e68a1577734b3bab76a67ae15fe40c",
      "d7226a0b5f0c4e2c90fd4ec1795ead31",
      "f2f0313a579444099bfa087c8ae8d711",
      "fc0ea0f521184d2ea037fca261fe1ade",
      "8d3f110c43b84aa991880abb36253961",
      "1184f66b876a4d1e89e7bfe70582dc27",
      "10f342d9aede49c4a2cf58e8a1133121",
      "49532120f7ae4144bdba0cbd47283545",
      "79aa894ddd5445ad83f41606a7c7832b",
      "b1b3350b3af446f1a93949f3af849139",
      "fe39300745ec49afa4e61c9851322d3d",
      "7d502f30325f4fe0951fc7de18903e77",
      "dd431253d44a4679aba4cba0de372f60",
      "87fc179919c44545b5c671dba3e90b62",
      "e6f3780cd87e4116902206189ab50aaa",
      "9f5c00efdedc4654a3b3283480698a61",
      "4015e821fd53462b9039b3664411d66b",
      "96357e42d9d14c8b96528ac8e2a22d46",
      "091037373efe480f9cd090b26b0fcf44",
      "b64767c571694caeaed62e7b66d0245d",
      "4f96fdb92e054d029da08f9bb40a8207",
      "70eae3ab89d9461facd957a536e47294",
      "17e76ea45ed145328bbcb7d5b5b1a3b5",
      "952638cb0609472d92380f24f36032dd",
      "dca0e3ab9ef3408a877e8d2073b26397",
      "9419111268b941a0bbb84ea8e5c48da3",
      "b2d700faafac48b79c9a7f87cd2075e0",
      "9274e80afeb5462da6fc5e146aa2613c",
      "f9131d7b900b496d8ef8bfd41465f9ba",
      "537bcd246f96439c8caca2eedfb85a9b",
      "4d1dfae38cf1414cba087cba4a861574",
      "05e5f5db36ac40c1abad8494773c8731",
      "4a438f20e8014beda2dde56bdcf3a9fa",
      "c3f0a785cc53472c9167a9e03cd70081",
      "3bc143c4f1dc477ab6158f063f397f1d",
      "1d30d682fc2e42feba544b65efc82401",
      "74179c5a3db54785af271e8b5a031840",
      "e663d118b013487885f004131120c413",
      "e1d3f7d466f04c65af0f26c77933f319",
      "53b55a35cde14d0eacd952abfdfe537a",
      "451b81af65834ac89a860c7a85daefb7",
      "b9596f61a295425f83ebbd4676c3c76f",
      "d6238bc117f94b41b2dc3840de2c68ae",
      "ddcf21c380e9418eb06b2a5ccef042b4",
      "a460d1686f604d06b56738d5e016aadd",
      "6f10d793fa434683bac5cb749e388ffe",
      "6e457d7f8c5149f39c044d461345e948",
      "a0cf542a2fe7467baaf555fde49dda8e",
      "dfdf652592ad4f7885a44766f426a67b",
      "f2b1bdf95bb84a8fba932ba5aae2c87b",
      "7c19a26d3342482289b1523a44e57a8a"
     ]
    },
    "id": "rCJZI4E0rFzA",
    "outputId": "7cc48c07-f61f-4d98-cb56-57ea7766298b"
   },
   "outputs": [],
   "source": [
    "# --- Visualization parameters ---\n",
    "vis_params = {\n",
    "    'bands': ['treecover2000'],\n",
    "    'min': 0,\n",
    "    'max': 100,\n",
    "    'palette': ['#ffffd9', '#edf8b1', '#c7e9b4', '#7fcdbb', '#41b6c4', '#1d91c0', '#225ea8', '#0c2c84']\n",
    "}\n",
    "\n",
    "loss_vis_params = {\n",
    "    'palette': ['#FF0000'] # Bright Red\n",
    "}\n",
    "\n",
    "\n",
    "# Show in interactive map\n",
    "Map = geemap.Map(center=[28.2, 84.0], zoom=7)\n",
    "\n",
    "# Add the tree cover layer to the map, clipping it to Nepal's borders\n",
    "Map.addLayer(\n",
    "    treecover2000.selfMask().clip(nepal_aoi), # Clip the image to our AOI\n",
    "    vis_params,                     # Apply our visualization style\n",
    "    'Tree Cover in 2000'            # Name the layer\n",
    ")\n",
    "\n",
    "# Border style\n",
    "Map.addLayer(nepal_aoi.style(**{'color': 'black', 'fillColor': '00000000'}), {}, 'Nepal Border')\n",
    "\n",
    "# Add the masked loss layer to our existing map object\n",
    "Map.addLayer(\n",
    "    loss.selfMask().clip(nepal_aoi), #  .selfMask() to make all '0' pixels transparent ensures that we only see the pixels where loss actually occurred.\n",
    "    loss_vis_params,\n",
    "    'Forest Loss (2000-2024)'\n",
    ")\n",
    "\n",
    "# Map.addLayer(gain, gain_vis, \"Forest Gain (2001-2024)\")\n",
    "Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsmrUzMo441e"
   },
   "source": [
    "### 2.3: Forest Area Statistics\n",
    "\n",
    "- Calculate **total forest area in 2000** using canopy cover threshold (â‰¥30%).  \n",
    "- Calculate **total forest loss** and **percentage loss** from 2001â€“2024.  \n",
    "- Generate **yearly forest loss statistics** for plotting.  \n",
    "- Bar chart shows **annual forest loss (mÂ²)** from 2001â€“2024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "U1rBqY5IrI8z",
    "outputId": "50a5786c-2f16-4ba8-b3b9-a8190e7cc24e"
   },
   "outputs": [],
   "source": [
    "# --- Compute forest area stats in mÂ² ---\n",
    "\n",
    "# Forest cover threshold (percent canopy)\n",
    "CANOPY_COVER_THRESHOLD = 30\n",
    "\n",
    "# Mask forest areas in 2000\n",
    "forest_2000 = treecover2000.gte(CANOPY_COVER_THRESHOLD)\n",
    "\n",
    "# Mask forest loss (only where it was forest in 2000)\n",
    "forest_loss = forest_2000.And(loss.eq(1))\n",
    "\n",
    "# Pixel area in mÂ²\n",
    "pixel_area_m2 = ee.Image.pixelArea()\n",
    "\n",
    "# Forest area\n",
    "forest_area_m2 = forest_2000.multiply(pixel_area_m2)\n",
    "forest_area_stats = forest_area_m2.reduceRegion(\n",
    "    reducer=ee.Reducer.sum(),\n",
    "    geometry=nepal_aoi.geometry(),\n",
    "    scale=30,\n",
    "    maxPixels=1e13\n",
    ")\n",
    "forest_area_m2_val = forest_area_stats.get('treecover2000').getInfo()\n",
    "\n",
    "# Forest loss area\n",
    "loss_area_m2 = forest_loss.multiply(pixel_area_m2)\n",
    "loss_area_stats = loss_area_m2.reduceRegion(\n",
    "    reducer=ee.Reducer.sum(),\n",
    "    geometry=nepal_aoi.geometry(),\n",
    "    scale=30,\n",
    "    maxPixels=1e13\n",
    ")\n",
    "loss_area_m2_val = loss_area_stats.get('treecover2000').getInfo()\n",
    "\n",
    "# Percentage loss\n",
    "percentage_loss = (loss_area_m2_val / forest_area_m2_val) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Forest cover (2000, >{CANOPY_COVER_THRESHOLD}% canopy): {forest_area_m2_val:,.0f} mÂ²\")\n",
    "print(f\"Forest loss (2001â€“2024): {loss_area_m2_val:,.0f} mÂ²\")\n",
    "print(f\"Percentage loss: {percentage_loss:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "acDxNJmx1JnT",
    "outputId": "17b9f229-d016-42cb-93b5-b016df568969"
   },
   "outputs": [],
   "source": [
    "# --- Compute yearly forest loss in mÂ² (2001-2024) ---\n",
    "\n",
    "years = list(range(2001, 2025))  # 2001 to 2024\n",
    "loss_per_year_m2 = []\n",
    "\n",
    "# Loop through each year\n",
    "for i, year in enumerate(years, start=1):  # Hansen \"lossyear\": 1 = 2001\n",
    "    # Mask pixels lost in this year AND were forest in 2000\n",
    "    loss_year_mask = forest_2000.And(lossyear.eq(i))\n",
    "\n",
    "    # Calculate area in mÂ²\n",
    "    loss_year_area_m2 = loss_year_mask.multiply(pixel_area_m2).reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=nepal_aoi.geometry(),\n",
    "        scale=30,\n",
    "        maxPixels=1e13\n",
    "    )\n",
    "\n",
    "    # Extract value safely\n",
    "    area_val = loss_year_area_m2.get('treecover2000').getInfo()\n",
    "    loss_per_year_m2.append(area_val if area_val else 0)\n",
    "\n",
    "# Convert to pandas DataFrame for plotting\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    \"Year\": years,\n",
    "    \"Forest Loss (mÂ²)\": loss_per_year_m2\n",
    "})\n",
    "\n",
    "print(loss_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "pB7JoAui1Ln8",
    "outputId": "7096bd97-2ad1-4543-c7d3-1a9d9159b84e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(loss_df[\"Year\"], loss_df[\"Forest Loss (mÂ²)\"], color='red', alpha=0.7)\n",
    "plt.title(\"Annual Forest Loss in Nepal (2001â€“2024)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Forest Loss (mÂ²)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfTf6Cx449f5"
   },
   "source": [
    "### 2.4: FarWest Region Forest Analysis\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- FAO GAUL Level 1 dataset used for administrative boundaries.  \n",
    "- Filter to select **Far Western Region of Nepal**.  \n",
    "- AOI used for all subsequent area calculations and maps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TwVkgf6d2F3g",
    "outputId": "30e9d0d9-3aa2-4e4b-c054-6c64d1f23281"
   },
   "outputs": [],
   "source": [
    "#  Define FarWest AOI\n",
    "admin_lvl_1 = ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
    "farwest_aoi = admin_lvl_1.filter(ee.Filter.And(\n",
    "    ee.Filter.eq('ADM0_NAME', 'Nepal'),\n",
    "    ee.Filter.eq('ADM1_NAME', 'Far Western')\n",
    "))\n",
    "farwest_geom = farwest_aoi.geometry()  # Geometry for calculations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVR69OOf5uBp"
   },
   "source": [
    "### 2.5: Forest, Loss, and Gain Masks\n",
    "\n",
    "- Forest mask: `treecover2000 â‰¥ 30%`.  \n",
    "- Forest loss mask: only areas that were forest in 2000 and lost since then.  \n",
    "- Forest gain mask: areas that gained forest since 2000.  \n",
    "- Masks will be used for **area calculations** and **visualization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "6E-pnBrn2GoA",
    "outputId": "48dc361d-a7d6-4485-9dfe-d0d5fe559c81"
   },
   "outputs": [],
   "source": [
    "# Load Hansen Global Forest Change dataset\n",
    "gfc = ee.Image('UMD/hansen/global_forest_change_2024_v1_12')\n",
    "CANOPY_COVER_THRESHOLD = 30\n",
    "pixel_area_m2 = ee.Image.pixelArea()\n",
    "\n",
    "# Masks\n",
    "is_forest_2000 = gfc.select('treecover2000').gte(CANOPY_COVER_THRESHOLD)\n",
    "has_loss = gfc.select('loss').eq(1)\n",
    "has_gain = gfc.select('gain').eq(1)\n",
    "\n",
    "# Forest loss only in areas that were forest in 2000\n",
    "actual_forest_loss = is_forest_2000.And(has_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOo35YjU51W4"
   },
   "source": [
    "### 2.6: Area Calculations\n",
    "\n",
    "- Compute forest area in 2000 (sq km).  \n",
    "- Compute total forest loss and gain (sq km).  \n",
    "- Calculate **net change** and **percentage change**.  \n",
    "- Summary table provides a clean overview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "WAyR1MOP2ITh",
    "outputId": "947872b8-18f9-45a5-aea1-0d80b56270ca"
   },
   "outputs": [],
   "source": [
    "# Calculate areas in sq km\n",
    "# Base forest area (2000)\n",
    "farwest_base_area = is_forest_2000.multiply(pixel_area_m2).reduceRegion(\n",
    "    reducer=ee.Reducer.sum(),\n",
    "    geometry=farwest_geom,\n",
    "    scale=30,\n",
    "    maxPixels=1e9\n",
    ").get('treecover2000').getInfo() / 1e6  # Convert mÂ² â†’ kmÂ²\n",
    "\n",
    "# Forest loss area\n",
    "farwest_loss_area = actual_forest_loss.multiply(pixel_area_m2).reduceRegion(\n",
    "    reducer=ee.Reducer.sum(),\n",
    "    geometry=farwest_geom,\n",
    "    scale=30,\n",
    "    maxPixels=1e9\n",
    ").get('treecover2000').getInfo() / 1e6\n",
    "\n",
    "# Forest gain area\n",
    "farwest_gain_area = has_gain.multiply(pixel_area_m2).reduceRegion(\n",
    "    reducer=ee.Reducer.sum(),\n",
    "    geometry=farwest_geom,\n",
    "    scale=30,\n",
    "    maxPixels=1e9\n",
    ").get('gain').getInfo() / 1e6\n",
    "\n",
    "# Net change\n",
    "farwest_net_change = farwest_gain_area - farwest_loss_area\n",
    "farwest_final_area = farwest_base_area + farwest_net_change\n",
    "farwest_net_percentage = (farwest_net_change / farwest_base_area) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "U2ln2hIA2J1d",
    "outputId": "b50d92df-175d-4d5f-c59c-9598f521e725"
   },
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Forest 2000 (sq km)\",\n",
    "        \"Forest Loss (sq km)\",\n",
    "        \"Forest Gain (sq km)\",\n",
    "        \"Net Change (sq km)\",\n",
    "        \"Final Forest Area (sq km)\",\n",
    "        \"Net % Change\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        farwest_base_area,\n",
    "        farwest_loss_area,\n",
    "        farwest_gain_area,\n",
    "        farwest_net_change,\n",
    "        farwest_final_area,\n",
    "        farwest_net_percentage\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYC9iRWs596k"
   },
   "source": [
    "### 2.6: Visualizations\n",
    " **Interactive map:** Overlay forest cover, loss, and gain for FarWest region.  \n",
    "   - Forest â†’ green  \n",
    "   - Loss â†’ red  \n",
    "   - AOI border â†’ black\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621,
     "referenced_widgets": [
      "2485617776104f319e1e853f7721db62",
      "0288287acd444469a503b96ca0b44dfd",
      "0d177be2d7f14bd8ad6faf0f3c353ae4",
      "a7d5792b3a0248ccbea8e1fc46da0361",
      "8663ab036fbd4768a687d126229e5d43",
      "c1491aff46d24c2ab4cad77a23385f92",
      "36ebc088958549aabd7d0bcb4aab27ae",
      "473523a92c144835acecd11b6292d134",
      "8e2dd4ab584a46c483c050a6543147ea",
      "298b85dbe8ee4b6db90d5fc445287531",
      "10d7c7379e2d4b4ea0978d801d90bd38",
      "0d401d07785b4d0f8fbd443e4b3cd8f1",
      "7e16e4364246411cb781948ef2e44ce3",
      "256c20fff7c948ff8688264a3fa22535",
      "2c00832c1e424c1d870c6a6c25a5acf7",
      "cc01ad35c8db4b8787300d2418be96a9",
      "c4cb29ca5fa144e8aae786cd06d65a6f",
      "7257b4e01d3346cba9262971047a7573",
      "3fb7b1c46b4d48c59436d0cad792217e",
      "91aa304fd0b94d809278b8653b11f0b8",
      "307ecc94417d4179b5613acba6087e39",
      "3140a89358d44910912bfce6ee37a567",
      "208fe98e29f247569315ec6c27c892b4",
      "92145e8f2a7e441ab8b699d4be7696ca",
      "25f85a87c49648279c191cc8f9331004",
      "0b8810a74e33497d8fb90e6625373695",
      "cbb6086553a849e8b459f0aec1d65d65",
      "252f8b143d5b4165948ff24bc8f32626",
      "5cb42dad4da04050bf5439e3814f9b01",
      "852c4710cdf5442394ea08ae3f643fde",
      "f55644ef28e14675a18de38927d247c1",
      "1b3881f1bbe0405984e4b34cc1fd88c2",
      "401dacaafbb74d4485cb6ba084821832",
      "0995f571f823426a971442b28494d671",
      "24570e26188146dfbce355f1f2b918a2",
      "7044588a130248128f8f0901a3b6c919",
      "e854ecb3b9a545f8823f0c570504941d",
      "9fbeaa9bbd2a4e9e89d6fb1eedf164ee",
      "f6ef28ea0a054f58b84a77b69f233861",
      "f70d93205ebb4062b0aad7115f3f7db7",
      "f09a04055b184b73b0b313889ceefd5b",
      "625db8720cd141b6b9e3c1bed33defaf",
      "65f68adf07844f1ca602c0c8630c39ca",
      "a565b01e72fe43cdbd4e835f26085553",
      "c80d36639466430ebf5ce7b385378bc3",
      "f9b9c623df1741c78a39a56766f16a1d",
      "8329aef3b98047a6b64587754f7c0515",
      "81e2a1578eac4dbeb2af361c1cf1247f",
      "bdadd84ca991453ea25ed1aa5e0d4c1d",
      "ce726114a6cd4626a6baf20059538770",
      "42688350e07849899e882258e417327a",
      "424ea707f7ae4e3798664a53e13bb5c9",
      "301c3eed8573449a88d1a1c0d0bfaf94",
      "9845030dd1ec4be18948e73d0c59e14b",
      "faaa03e91feb4fa8b0c71401fa434c9d",
      "d1674f8cf1ef45ca9bdc8f6ad245e0d4",
      "05e43a27907b4cfba8e2942afa9190e8",
      "9f85fa2b0d3f4ea191ecfbacfc7bf327",
      "1065fbdd24434b5e9aa3f4360c326216",
      "64ba839e31e94cd9b08713822373b7a5",
      "45a6ea4be5f943cba5372e2e6ec8f487",
      "7023418c574c42349efcf5288ec53f4a",
      "0beca020d5d14206a7319ba022151e57",
      "28105a9723bf4b35b4ae2f13d8d63a1a",
      "15399beadd8046f1a4968bf827c31a82",
      "cc00e72592304fec8aa808d821122cc3",
      "833620bff10a434d933f7290e3b91e4a",
      "63857742aaa3491ea2153d7ff23f87ac",
      "d1193d0a16db4ec3a40efb8982d0a81d",
      "22a11bbbab3243179afaa5479b637e3a",
      "8a34e555567446478a98a2fcb7bab044",
      "08e25760f8cc486baf94d87d229d6534",
      "427919906cd046aa9bac445f33f087a4",
      "ae7a01b8cdeb44ce8a8d3ad1462a6019",
      "a81577ff9ca944069ea3ba3a698ae908",
      "20b21e8a0de4464a9a5accc364e09293",
      "0a5a5e99ea204e219ef635cd3e911c64",
      "3c427b6b2e574ba382be3921bc04c8d2",
      "702976ba4e684213a3ed4c8646f4837e",
      "90be21c74a504f588373f4d4ab9b2aa2",
      "55b1f889147741f4a61b54fd885abada",
      "abeffed05ad34ac3889e158f7e6dc076",
      "9b44d8071ab7464a8d153838e0934357",
      "0656c48b26f44792906650607a66a0c5",
      "ed310fa24ebe44f58bbcef5ff5a0ae95",
      "06e4ecfa71154363acaa7e706568243a",
      "f290f96581524f768f1dfb3832606615",
      "7c7df4aa3a1046ce8bf65096eac35bcb",
      "62d8cb86a1a4407aa23bdc363ddcb4b0",
      "b1e67015c8f24a80a3dd8b28dd583d36",
      "e3c84ab74d17430ab22fd3d7237455cb",
      "d9aec7ffa6e3403485e11224a4589c97",
      "11a00b2737934205b02f9a9c9bba3426",
      "a877a67567f645bbbf8ab4ad1a2ba8a2",
      "95c0acb4cd394ca0ac2e87072ede0b8b",
      "914536170a13456890bcee92c0366fa2",
      "83dfff273baa4f7388c8a463d2d0aa76",
      "93345f6ad0d9478cb0a696b69a3f519f",
      "e9828634b7554c1ab8071161b25ac752",
      "d7ef159d9dea4765bed3fe79cabfa5ab",
      "7baddf6e267d4d2c8a5ee491baa5a5f3",
      "982c2e52e6be49c8b7977bdc43517346",
      "548af7220d6542c5852a269aab4da283",
      "8059170fa4d44c0bad7f95f5ee494d34",
      "b03a3ed1119e4965af78da8e9466dc59"
     ]
    },
    "id": "h21aSOjp2MHw",
    "outputId": "f32bb6b2-467f-448e-f66c-db9906fca1c7"
   },
   "outputs": [],
   "source": [
    "# --- 6. Interactive Map (FarWest) using earlier palettes ---\n",
    "map_farwest = geemap.Map(center=[28.5, 80.5], zoom=9)\n",
    "\n",
    "# Forest cover 2000 â†’ use same vis_params as Nepal\n",
    "map_farwest.addLayer(is_forest_2000.selfMask().clip(farwest_geom), vis_params, 'Forest Cover 2000')\n",
    "\n",
    "# Forest loss â†’ reuse earlier loss_vis_params (red palette)\n",
    "map_farwest.addLayer(actual_forest_loss.selfMask().clip(farwest_geom), loss_vis_params, 'Forest Loss')\n",
    "\n",
    "# AOI border\n",
    "map_farwest.addLayer(farwest_aoi.style(color='black', fillColor='00000000'), {}, 'FarWest Border')\n",
    "\n",
    "map_farwest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHJDmUARQqJ3"
   },
   "source": [
    "---\n",
    "## 3. Deep Learning Data Preparation\n",
    "\n",
    "With our foundational analysis complete, we now prepare the data for our deep learning models. This section covers the entire pipeline, from defining our data sources in Google Earth Engine to creating the final PyTorch DataLoaders that will feed the models during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yg5CBU4QuV1"
   },
   "source": [
    "### 3.1. Defining Data Sources in GEE\n",
    "\n",
    "We will define two sets of input data for our comparative analysis: a baseline \"static\" image and an \"enhanced\" image that includes topographic data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiIk2kfpRG5P"
   },
   "source": [
    "#### 3.1.1. Static Data (3-Class Problem)\n",
    "\n",
    "Our first set of experiments will use a 4-channel satellite image as input. The goal is to train a model to predict a 3-class historical land cover map.\n",
    "\n",
    "*   **Input Image (X):** A 4-channel image from a recent, cloud-free Sentinel-2 composite (Red, Green, Blue, Near-Infrared).\n",
    "*   **Label Mask (y):** Our 3-class \"ground truth\" layer derived from the Hansen dataset: `0` for Stable Non-Forest, `1` for Stable Forest (since 2000), and `2` for Deforested Land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2ttr5u5eMd-X",
    "outputId": "ba36b78e-0750-44f3-9662-d4ca54dbbb21"
   },
   "outputs": [],
   "source": [
    "# --- 3.1.1: Define Static Data Sources ---\n",
    "\n",
    "# Define a recent, cloud-free Sentinel-2 image composite\n",
    "s2_image_static = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "                     .filterBounds(nepal_aoi)\n",
    "                     .filterDate('2023-10-01', '2023-12-31')\n",
    "                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "                     .median()\n",
    "                     .select(['B4', 'B3', 'B2', 'B8'])) # R, G, B, NIR\n",
    "\n",
    "# Define the 3-class historical label\n",
    "label_static = (ee.Image(0) # Start with class 0 (Non-Forest)\n",
    "                .where(gfc.select('treecover2000').gte(30), 1) # Add class 1 (Forest)\n",
    "                .where(gfc.select('treecover2000').gte(30).And(gfc.select('loss').eq(1)), 2) # Add class 2 (Deforested)\n",
    "                .rename('label'))\n",
    "\n",
    "print(\"Static 4-channel input image and 3-class label defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkjxtMlRRC6A"
   },
   "source": [
    "#### 3.1.2. Enhanced Data with Topography\n",
    "\n",
    "To test our hypothesis that geographical context can improve model performance, we create an enhanced input image. We fuse the 4-channel Sentinel-2 imagery with 2 static topographic channels derived from a Digital Elevation Model (DEM).\n",
    "\n",
    "*   **Enhanced Input Image (X):** A 6-channel image (R, G, B, NIR, Elevation, Slope).\n",
    "*   **Label Mask (y):** The same 3-class label is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DBUDAZkJRAyk",
    "outputId": "24b721df-282d-4dde-832c-868fb0be9b6a"
   },
   "outputs": [],
   "source": [
    "# --- 3.1.2: Define Enhanced Data Sources with Topography ---\n",
    "\n",
    "# Load the Digital Elevation Model (DEM) and calculate slope\n",
    "dem = ee.Image('USGS/SRTMGL1_003')\n",
    "elevation = dem.select('elevation')\n",
    "slope = ee.Terrain.slope(dem)\n",
    "\n",
    "# Create the 6-channel input image by stacking all bands\n",
    "input_image_with_topo = s2_image_static.addBands(elevation).addBands(slope)\n",
    "\n",
    "print(\"Enhanced 6-channel input image created.\")\n",
    "print(\"Bands:\", input_image_with_topo.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1p_qTvvRNKO"
   },
   "source": [
    "### 3.2. Exporting Training Datasets\n",
    "\n",
    "With our data sources defined, we now export the labeled image patches that will form our training datasets. The following cells are for this one-time, time-intensive data export. They use a \"smart\" resumable logic that checks for existing files and only exports the missing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTT7kECjRQef"
   },
   "source": [
    "#### 3.2.1. Exporting Static Patches\n",
    "\n",
    "These are the 5-band GeoTIFFs (4 input channels + 1 label) for our baseline experiments, saved to the `data/sat_patch` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "mEhqIWZfRRDu",
    "outputId": "7681c1c0-5103-4333-85f8-0dc8d94764da"
   },
   "outputs": [],
   "source": [
    "# Define Export Parameters ---\n",
    "STATIC_FOLDER = '/content/drive/My Drive/NEPAL_DEFORESTATION/data/sat_patch'\n",
    "if not os.path.exists(STATIC_FOLDER): os.makedirs(STATIC_FOLDER)\n",
    "print(f\"Exporting to static data folder: {STATIC_FOLDER}\")\n",
    "\n",
    "TOTAL_SAMPLES = 5000\n",
    "PATCH_SIZE = 64\n",
    "SCALE = 10\n",
    "FILE_PREFIX = 'nepal_forest_patch'\n",
    "\n",
    "# Check for Already Exported Files to prevent reduncancy\n",
    "print(\"Checking for existing static files...\")\n",
    "try:\n",
    "    existing_files = os.listdir(STATIC_FOLDER)\n",
    "    existing_indices = {int(f.split('_')[-1].split('.')[0]) for f in existing_files if f.startswith(FILE_PREFIX)}\n",
    "    print(f\"Found {len(existing_indices)} files already exported.\")\n",
    "except FileNotFoundError:\n",
    "    existing_indices = set()\n",
    "\n",
    "# Generate and Fetch Sample Points\n",
    "all_indices = set(range(TOTAL_SAMPLES))\n",
    "missing_indices = sorted(list(all_indices - existing_indices))\n",
    "\n",
    "if not missing_indices:\n",
    "    print(\"\\nStatic dataset export is already complete!\")\n",
    "else:\n",
    "    print(f\"\\n{len(missing_indices)} files are missing. Preparing to generate points and launch export tasks.\")\n",
    "\n",
    "    # Generate ALL Potential Sample Points on the server\n",
    "    sample_points = ee.FeatureCollection.randomPoints(\n",
    "        region=nepal_aoi.geometry(),\n",
    "        points=TOTAL_SAMPLES,\n",
    "        seed=42\n",
    "    )\n",
    "    # Fetch ALL point geometries to the client\n",
    "    print(\"Fetching all point geometries... (This may take a moment)\")\n",
    "    points_info = sample_points.getInfo()['features']\n",
    "    print(f\"Successfully fetched {len(points_info)} concrete point geometries.\")\n",
    "\n",
    "    # Define the export image\n",
    "    export_image_static = s2_image_static.addBands(label_static).toFloat()\n",
    "\n",
    "    # Export loop\n",
    "    print(f\"Launching {len(missing_indices)} new export tasks... This may take a while.\")\n",
    "\n",
    "    for i in missing_indices:\n",
    "        feature = points_info[i]\n",
    "        filename = f'{FILE_PREFIX}_{i:04d}'\n",
    "\n",
    "        point_geometry = feature['geometry']\n",
    "\n",
    "        export_region = ee.Geometry.Point(\n",
    "            point_geometry['coordinates']).buffer(PATCH_SIZE * SCALE / 2).bounds()\n",
    "\n",
    "        image_to_export = export_image_static.clip(export_region)\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=image_to_export,\n",
    "            description=f'Static_Export_{filename}',\n",
    "            folder=os.path.basename(os.path.dirname(STATIC_FOLDER)),\n",
    "            fileNamePrefix=f\"{os.path.basename(STATIC_FOLDER)}/{filename}\",\n",
    "            region=export_region.getInfo()['coordinates'],\n",
    "            scale=SCALE,\n",
    "            fileFormat='GeoTIFF'\n",
    "        )\n",
    "        task.start()\n",
    "\n",
    "    print(f\"\\nSuccessfully LAUNCHED {len(missing_indices)} new export tasks.\")\n",
    "    print(\"Please go to the GEE Tasks tab to monitor and run them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8bjWejJSlpS"
   },
   "source": [
    "#### 3.2.2. Exporting Topo-Enhanced Patches\n",
    "\n",
    "These are the 7-band GeoTIFFs (6 input channels + 1 label) for our advanced model experiments, saved to the `data/sat_patch_topo` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "OCe5azdFSrMv",
    "outputId": "0c2413ba-5675-4e1d-9e34-abdc79c858a2"
   },
   "outputs": [],
   "source": [
    "# Export Parameters\n",
    "TOPO_FOLDER = '/content/drive/My Drive/NEPAL_DEFORESTATION/data/sat_patch_topo'\n",
    "if not os.path.exists(TOPO_FOLDER): os.makedirs(TOPO_FOLDER)\n",
    "print(f\"Exporting to topo-enhanced data folder: {TOPO_FOLDER}\")\n",
    "\n",
    "TOTAL_SAMPLES = 5000\n",
    "PATCH_SIZE = 64\n",
    "SCALE = 10\n",
    "FILE_PREFIX = 'nepal_topo_patch' # unique prefix for this dataset\n",
    "\n",
    "# Check for Already Exported Files\n",
    "print(\"Checking for existing topo-enhanced files...\")\n",
    "try:\n",
    "    existing_files = os.listdir(TOPO_FOLDER)\n",
    "    existing_indices = {int(f.split('_')[-1].split('.')[0]) for f in existing_files if f.startswith(FILE_PREFIX)}\n",
    "    print(f\"Found {len(existing_indices)} files already exported.\")\n",
    "except FileNotFoundError:\n",
    "    existing_indices = set()\n",
    "\n",
    "# Generate and Fetch Sample Points\n",
    "all_indices = set(range(TOTAL_SAMPLES))\n",
    "missing_indices = sorted(list(all_indices - existing_indices))\n",
    "\n",
    "if not missing_indices:\n",
    "    print(\"\\nTopo-enhanced dataset export is already complete!\")\n",
    "else:\n",
    "    print(f\"\\n{len(missing_indices)} files are missing. Preparing to generate points and launch export tasks.\")\n",
    "\n",
    "    # Generate ALL Potential Sample Points on the server\n",
    "    # We use the same points as the static export for a fair comparison\n",
    "    sample_points = ee.FeatureCollection.randomPoints(\n",
    "        region=nepal_aoi.geometry(),\n",
    "        points=TOTAL_SAMPLES,\n",
    "        seed=42\n",
    "    )\n",
    "    # Fetch ALL point geometries to the client\n",
    "    print(\"Fetching all point geometries... (This may take a moment)\")\n",
    "    points_info = sample_points.getInfo()['features']\n",
    "    print(f\"Successfully fetched {len(points_info)} concrete point geometries.\")\n",
    "\n",
    "    # Define the export image\n",
    "    # This uses the 'input_image_with_topo' and 'label_static' from cell 3.1.2\n",
    "    export_image_topo = input_image_with_topo.addBands(label_static).toFloat()\n",
    "\n",
    "    # Export loop\n",
    "    print(f\"Launching {len(missing_indices)} new export tasks... This may take a while.\")\n",
    "\n",
    "    for i in missing_indices:\n",
    "        feature = points_info[i]\n",
    "        filename = f'{FILE_PREFIX}_{i:04d}'\n",
    "\n",
    "        point_geometry = feature['geometry']\n",
    "\n",
    "        export_region = ee.Geometry.Point(\n",
    "            point_geometry['coordinates']).buffer(PATCH_SIZE * SCALE / 2).bounds()\n",
    "\n",
    "        image_to_export = export_image_topo.clip(export_region)\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=image_to_export,\n",
    "            description=f'Topo_Export_{filename}',\n",
    "            folder=os.path.basename(os.path.dirname(TOPO_FOLDER)),\n",
    "            fileNamePrefix=f\"{os.path.basename(TOPO_FOLDER)}/{filename}\",\n",
    "            region=export_region.getInfo()['coordinates'],\n",
    "            scale=SCALE,\n",
    "            fileFormat='GeoTIFF'\n",
    "        )\n",
    "        task.start()\n",
    "\n",
    "    print(f\"\\nSuccessfully LAUNCHED {len(missing_indices)} new export tasks.\")\n",
    "    print(\"Please go to the GEE Tasks tab to monitor and run them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVKM3EXzVw_R"
   },
   "source": [
    "### 3.3. Creating the PyTorch Dataset Classes\n",
    "\n",
    "With our image patches exported, we now define our PyTorch `Dataset` classes to read them. Because our experiments use two different types of input data (4-channel static vs. 6-channel topographic), we will create two distinct `Dataset` classes. This keeps our data pipelines clean and explicit. Both classes incorporate the robust `NoData` handling we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MJh5U5apVxvx",
    "outputId": "3ec62834-50bc-48b6-df29-d3cb9ccfcfe2"
   },
   "outputs": [],
   "source": [
    "# A global constant for the ignore index.\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "class StaticDataset(Dataset):\n",
    "    \"\"\"Dataset for the 5-band (4 input + 1 label) static GeoTIFFs.\"\"\"\n",
    "    def __init__(self, root_dir, target_size=64):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.tif')]\n",
    "\n",
    "    def __len__(self): return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        with rasterio.open(img_path) as dataset:\n",
    "            numpy_array = dataset.read(masked=True).filled(fill_value=IGNORE_INDEX)\n",
    "        _, h, w = numpy_array.shape\n",
    "        if h > self.target_size or w > self.target_size:\n",
    "            top = (h - self.target_size) // 2; left = (w - self.target_size) // 2\n",
    "            numpy_array = numpy_array[:, top:top + self.target_size, left:left + self.target_size]\n",
    "        image_numpy = numpy_array[:4, :, :]; label_numpy = numpy_array[4, :, :]\n",
    "        return torch.from_numpy(image_numpy).to(torch.float32), torch.from_numpy(label_numpy).to(torch.long)\n",
    "\n",
    "class TopoDataset(Dataset):\n",
    "    \"\"\"Dataset for the 7-band (6 input + 1 label) topo-enhanced GeoTIFFs.\"\"\"\n",
    "    def __init__(self, root_dir, target_size=64):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.tif')]\n",
    "\n",
    "    def __len__(self): return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        with rasterio.open(img_path) as dataset:\n",
    "            numpy_array = dataset.read(masked=True).filled(fill_value=IGNORE_INDEX)\n",
    "        _, h, w = numpy_array.shape\n",
    "        if h > self.target_size or w > self.target_size:\n",
    "            top = (h - self.target_size) // 2; left = (w - self.target_size) // 2\n",
    "            numpy_array = numpy_array[:, top:top + self.target_size, left:left + self.target_size]\n",
    "        image_numpy = numpy_array[:6, :, :]; label_numpy = numpy_array[6, :, :]\n",
    "        return torch.from_numpy(image_numpy).to(torch.float32), torch.from_numpy(label_numpy).to(torch.long)\n",
    "\n",
    "print(\"Dataset classes defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_SoCAzyV7th"
   },
   "source": [
    "### 3.4. Loading and Splitting the Datasets\n",
    "\n",
    "Now, we instantiate our `Dataset` classes for both the static and topo-enhanced data folders. We then split each full dataset into Training (70%), Validation (15%), and Test (15%) sets. Using a fixed random seed ensures that these splits are identical every time we run the notebook, which is crucial for reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "opEf6kLrV966",
    "outputId": "1a032d8b-0081-4c15-fcb4-65ca8fdeec22"
   },
   "outputs": [],
   "source": [
    "# --- Create and Split the STATIC Dataset ---\n",
    "STATIC_FOLDER = DATA_DIR / \"sat_patch\"\n",
    "print(f\"Loading static dataset from: {STATIC_FOLDER}\")\n",
    "static_full_dataset = StaticDataset(root_dir=STATIC_FOLDER)\n",
    "print(f\"Loaded {len(static_full_dataset)} total static samples.\")\n",
    "\n",
    "train_size_static = int(0.7 * len(static_full_dataset))\n",
    "val_size_static = int(0.15 * len(static_full_dataset))\n",
    "test_size_static = len(static_full_dataset) - train_size_static - val_size_static\n",
    "\n",
    "static_train_dataset, static_val_dataset, static_test_dataset = random_split(\n",
    "    static_full_dataset, [train_size_static, val_size_static, test_size_static],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Split static dataset into: Train ({len(static_train_dataset)}), Val ({len(static_val_dataset)}), Test ({len(static_test_dataset)})\")\n",
    "\n",
    "# --- Create and Split the TOPO-ENHANCED Dataset ---\n",
    "TOPO_FOLDER = DATA_DIR / \"sat_patch_topo\"\n",
    "print(f\"\\nLoading topo-enhanced dataset from: {TOPO_FOLDER}\")\n",
    "topo_full_dataset = TopoDataset(root_dir=TOPO_FOLDER)\n",
    "print(f\"Loaded {len(topo_full_dataset)} total topo-enhanced samples.\")\n",
    "\n",
    "train_size_topo = int(0.7 * len(topo_full_dataset))\n",
    "val_size_topo = int(0.15 * len(topo_full_dataset))\n",
    "test_size_topo = len(topo_full_dataset) - train_size_topo - val_size_topo\n",
    "\n",
    "topo_train_dataset, topo_val_dataset, topo_test_dataset = random_split(\n",
    "    topo_full_dataset, [train_size_topo, val_size_topo, test_size_topo],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Split topo-enhanced dataset into: Train ({len(topo_train_dataset)}), Val ({len(topo_val_dataset)}), Test ({len(topo_test_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1D5wkI6WEcW"
   },
   "source": [
    "### 3.5. Calculating Class Weights for the Loss Function\n",
    "\n",
    "Our analysis of the Hansen dataset shows that the \"Deforested\" class is extremely rare. To address this severe class imbalance, we calculate class weights based on the inverse frequency of each class in the **training data**. This will force our models to pay significantly more attention to the rare but critical \"Deforested\" class during training.\n",
    "\n",
    "The following cell calculates these weights by iterating through the training set once. To save time on future runs, it saves the calculated weights tensor to a file (`class_weights.pth`) and will load it directly if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "Iz4aQc3UWHZ8",
    "outputId": "19eb929f-7125-4aa8-e7ab-ba776e517cb1"
   },
   "outputs": [],
   "source": [
    "# Define the file path ---\n",
    "# Path points to the 'model' subfolder as requested.\n",
    "model_subfolder = MODEL_DIR\n",
    "WEIGHTS_FILE_PATH = os.path.join(model_subfolder, 'class_weights.pth')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ensure the 'model' subfolder exists\n",
    "os.makedirs(model_subfolder, exist_ok=True)\n",
    "\n",
    "# Check if the weights file already exists\n",
    "if os.path.exists(WEIGHTS_FILE_PATH):\n",
    "    # If it exists, load it and skip the calculation\n",
    "    print(f\"Found existing weights file. Loading from: {WEIGHTS_FILE_PATH}\")\n",
    "    class_weights_tensor = torch.load(WEIGHTS_FILE_PATH, map_location=device)\n",
    "\n",
    "else:\n",
    "    # If it does not exist, perform the full calculation and save the result\n",
    "    print(\"Weights file not found. Calculating class frequencies...\")\n",
    "\n",
    "    # We use the static_train_dataset to calculate the weights.\n",
    "    weight_loader = DataLoader(static_train_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "    class_pixel_counts = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "    print(\"Counting pixels for each class... (This may take a moment)\")\n",
    "    for _, labels in weight_loader:\n",
    "        valid_labels = labels[labels != IGNORE_INDEX]\n",
    "        unique, counts = np.unique(valid_labels.numpy(), return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            if u in class_pixel_counts:\n",
    "                class_pixel_counts[u] += c\n",
    "\n",
    "    print(\"\\n--- Class Distribution Analysis (3-Class) ---\")\n",
    "    total_pixels = sum(class_pixel_counts.values())\n",
    "    class_names = [\"0: Non-Forest\", \"1: Forest\", \"2: Deforested\"]\n",
    "    class_weights = []\n",
    "\n",
    "    for i in range(len(class_names)):\n",
    "        frequency = class_pixel_counts[i] / total_pixels\n",
    "        weight = 1.0 / (frequency + 1e-6)\n",
    "        class_weights.append(weight)\n",
    "        print(f\"  - {class_names[i]}: {class_pixel_counts[i]:,} pixels ({frequency:.4%})\")\n",
    "\n",
    "    class_weights = np.array(class_weights)\n",
    "    class_weights = class_weights / np.sum(class_weights)\n",
    "\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    print(f\"\\nSaving calculated class weights to: {WEIGHTS_FILE_PATH}\")\n",
    "    torch.save(class_weights_tensor, WEIGHTS_FILE_PATH)\n",
    "\n",
    "# --- Final Output ---\n",
    "print(\"\\n--- Final Calculated Weights for 3-Class Problem ---\")\n",
    "print(class_weights_tensor)\n",
    "print(f\"Tensor is on device: {class_weights_tensor.device}\")\n",
    "print(\"\\nClass weights are ready for use in our weighted loss function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OKTnXoIAwh6"
   },
   "source": [
    "## 4. Deep Learning Tools: Architectures and Master Functions\n",
    "\n",
    "With our data pipelines ready, we now define the core components of our deep learning experiments. This section contains the class definitions for our model architectures (`SimpleFCN`, `UNet`) and the master functions for training, evaluation, and visualization. This modular approach allows us to run multiple experiments cleanly and consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YU-XHgAEA-E3"
   },
   "source": [
    "### 4.1. Designing the Baseline CNN Model (SimpleFCN)\n",
    "\n",
    "Our first model is a Simple Fully Convolutional Network (FCN). This is a standard CNN architecture that preserves spatial dimensions from input to output, making it suitable for semantic segmentation. It consists of several blocks of `Convolution -> Batch Normalization -> ReLU` layers that extract features, followed by a final `1x1 convolution` that acts as a pixel-wise classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "InDyVSbaAu2E",
    "outputId": "5d57eddf-062b-482c-f7dd-b68a1c1441ae"
   },
   "outputs": [],
   "source": [
    "class simpleFCN(nn.Module):\n",
    "  def __init__(self, in_channels, num_classes):\n",
    "      \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input image (4 for us: R,G,B,NIR).\n",
    "            num_classes (int): Number of output classes (3 for us: Non-Forest, Forest, Deforested).\n",
    "      \"\"\"\n",
    "      super(simpleFCN, self).__init__()\n",
    "\n",
    "      # --- The \"Encoder\" Path ---\n",
    "      # We create a series of blocks that extract features.\n",
    "      # The padding='same' argument is crucial ensures that the output  height and width are the same as the input height and width.\n",
    "\n",
    "      # Block 1: Input -> 16 channels\n",
    "      self.block1 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels, 16, kernel_size=3, padding='same'),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(inplace=True)\n",
    "      )\n",
    "\n",
    "      # Block 2: 16 -> 32 channels\n",
    "      self.block2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, kernel_size=3, padding='same'),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(inplace=True)\n",
    "      )\n",
    "\n",
    "      # Block 3: 32 -> 64 channels\n",
    "      self.block3 = nn.Sequential(\n",
    "          nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(inplace=True)\n",
    "      )\n",
    "\n",
    "      # The Final Classifier Layer\n",
    "      # This 1x1 convolution acts as a pixel-wise classifier.\n",
    "      # It takes the 64 feature channels and condenses them down to\n",
    "      # 3 output channels, one for each class score.\n",
    "      self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "      \"\"\"The forward pass defines how data flows through the network.\"\"\"\n",
    "      # Pass input through the encoder blocks\n",
    "      x = self.block1(x)\n",
    "      x = self.block2(x)\n",
    "      x = self.block3(x)\n",
    "\n",
    "      # Pass through the final classifier\n",
    "      x = self.final_conv(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XflSuc1sBF2R"
   },
   "source": [
    "### 4.2. Designing an Advanced Architecture (U-Net)\n",
    "\n",
    "Our analysis of the baseline FCN revealed its limitations, particularly in producing spatially precise prediction maps. To address this, we implement a more advanced and powerful architecture: the **U-Net**.\n",
    "\n",
    "The U-Net is the industry standard for semantic segmentation, especially in medical and satellite imagery. Its architecture is composed of two distinct paths:\n",
    "\n",
    "1.  **The Encoder (Contracting Path):** This path acts like a feature extractor. It uses a series of convolutional and max pooling layers to progressively downsample the image. This allows the model to learn the high-level contextual features of the imageâ€”the \"what\" (e.g., \"this is a forest texture\").\n",
    "\n",
    "2.  **The Decoder (Expansive Path):** This path symmetrically upsamples the feature maps, using transposed convolutions to gradually rebuild the image back to its original resolution. This allows the model to learn the precise location of the featuresâ€”the \"where\".\n",
    "\n",
    "The most critical innovation of the U-Net is the use of **skip connections**. These connections feed the high-resolution feature maps from the encoder directly to the corresponding layers in the decoder. This process allows the decoder to use both the high-level context and the fine-grained spatial details to produce exceptionally clean and precise segmentation masks, directly addressing the \"noisy prediction\" problem of the simpler FCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9W71f39lBGVP",
    "outputId": "ef1a7033-820f-4497-8138-c97853fe7a42"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"A helper module with two convolutions: (conv => BN => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"The U-Net architecture for semantic segmentation.\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # --- Encoder (Downsampling Path) ---\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
    "\n",
    "        # --- Decoder (Upsampling Path) ---\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv1 = DoubleConv(512, 256) # 256 from upsample + 256 from skip\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(256, 128) # 128 from upsample + 128 from skip\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(128, 64) # 64 from upsample + 64 from skip\n",
    "\n",
    "        # --- Final Classifier Layer ---\n",
    "        self.outc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)     # -> 64x64x64\n",
    "        x2 = self.down1(x1)  # -> 32x32x128\n",
    "        x3 = self.down2(x2)  # -> 16x16x256\n",
    "        x4 = self.down3(x3)  # -> 8x8x512\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x4)      # Upsample to 16x16x256\n",
    "        # Concatenate with skip connection from encoder\n",
    "        x = torch.cat([x3, x], dim=1) # -> 16x16x512\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.up2(x)      # Upsample to 32x32x128\n",
    "        x = torch.cat([x2, x], dim=1) # -> 32x32x256\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.up3(x)      # Upsample to 64x64x64\n",
    "        x = torch.cat([x1, x], dim=1) # -> 64x64x128\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Final output\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvuwDo6hEQyq"
   },
   "source": [
    "#### 4.3. Baseline Dataset Class\n",
    "\n",
    "To create a bridge between our raw `.tif` files on disk and the PyTorch framework, we define a custom `Dataset` class. The `ForestDataset` class is responsible for handling the 5-band static (non-topographic) image patches.\n",
    "\n",
    "Its key responsibilities include:\n",
    "*   Locating all the `.tif` files in the data directory.\n",
    "*   Reading a single 5-band GeoTIFF file (4 input channels + 1 label) using the `rasterio` library.\n",
    "*   **Robustly handling \"NoData\" pixels:** It reads the data as a masked array and uses the `.filled()` method to replace any invalid pixels with our designated `IGNORE_INDEX`. This was a critical step identified during our debugging process to prevent training errors.\n",
    "*   Performing a center crop to ensure all samples are a uniform size for batching.\n",
    "*   Separating the data into a 4-channel image and a 1-channel label, and converting them into the appropriate PyTorch tensors.\n",
    "\n",
    "This class will serve as the data pipeline for our initial baseline FCN experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "G3ftADMNES88",
    "outputId": "f1e057a8-c441-49d1-ed1b-714c8d7a8e98"
   },
   "outputs": [],
   "source": [
    "class ForestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the 5-band (4 input + 1 label) static GeoTIFFs.\n",
    "    This is used for our baseline FCN experiments.\n",
    "    It robustly handles NoData values and performs center cropping.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, target_size=64):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_size = target_size\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.tif')]\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        with rasterio.open(img_path) as dataset:\n",
    "            # The robust way to handle potential NoData values\n",
    "            numpy_array = dataset.read(masked=True).filled(fill_value=IGNORE_INDEX)\n",
    "\n",
    "        _, h, w = numpy_array.shape\n",
    "        if h > self.target_size or w > self.target_size:\n",
    "            top = (h - self.target_size) // 2; left = (w - self.target_size) // 2\n",
    "            numpy_array = numpy_array[:, top:top + self.target_size, left:left + self.target_size]\n",
    "\n",
    "        image_numpy = numpy_array[:4, :, :]\n",
    "        label_numpy = numpy_array[4, :, :]\n",
    "\n",
    "        return torch.from_numpy(image_numpy).to(torch.float32), torch.from_numpy(label_numpy).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2_o-nF5FXFn"
   },
   "source": [
    "### 4.4. Master Functions for a Modular Workflow\n",
    "\n",
    "To ensure our experiments are consistent, reproducible, and that our code is clean and efficient, we define a set of master functions. These functions encapsulate the core logic for training, evaluation, and visualization, and they will be used by all subsequent experiments.\n",
    "\n",
    "**1. `train_model` function:**\n",
    "This is the core engine of our project. It handles the entire training and validation loop for a given model. Its key features include:\n",
    "*   A standard training loop that iterates through epochs and batches.\n",
    "*   A validation loop to monitor performance on unseen data after each epoch.\n",
    "*   **Stateful Checkpointing:** It saves the complete state (model weights, optimizer state, epoch number, etc.) after every epoch, allowing training to be resumed seamlessly after an interruption.\n",
    "*   **Early Stopping:** It monitors the validation loss and automatically stops the training if performance does not improve for a set number of epochs (`patience`), preventing overfitting and saving time.\n",
    "*   **Learning Rate Scheduling:** It incorporates a `ReduceLROnPlateau` scheduler to automatically adjust the learning rate for more stable convergence.\n",
    "\n",
    "**2. `evaluate_model` function:**\n",
    "This function is responsible for the **quantitative analysis** of a trained model. It takes a model and a test dataloader, runs inference on all samples, and calculates a comprehensive suite of performance metrics, including:\n",
    "*   Overall Pixel Accuracy\n",
    "*   Per-Class Precision, Recall, F1-Score, and Intersection over Union (IoU)\n",
    "*   Macro-Averaged summary metrics (mIoU and Macro F1-Score)\n",
    "\n",
    "It also programmatically stores a curated summary of these results in our master `all_results` dictionary for the final comparison.\n",
    "\n",
    "**3. `visualize_predictions` function:**\n",
    "This function handles the **qualitative analysis**. It takes a trained model and a dataset, selects a few random samples, and plots three images side-by-side:\n",
    "*   The original satellite image input.\n",
    "*   The ground truth label map.\n",
    "*   The model's final prediction map.\n",
    "\n",
    "This provides an intuitive, visual understanding of the model's performance and its specific failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ONGwiZIYFY4M",
    "outputId": "63e887a0-0374-489a-b20e-81653fa4e598"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader,\n",
    "                scheduler, num_epochs, patience, checkpoint_path):\n",
    "    \"\"\"\n",
    "    A master function to handle the complete training and validation loop.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to train.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler. Can be None.\n",
    "        num_epochs (int): The maximum number of epochs to train for.\n",
    "        patience (int): The patience for early stopping.\n",
    "        checkpoint_path (str): The file path to save/load checkpoints.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - model (torch.nn.Module): The model with the best weights loaded.\n",
    "            - train_loss_history (list): A list of average training losses per epoch.\n",
    "            - val_loss_history (list): A list of average validation losses per epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Starting training on device: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Checkpointing Logic\n",
    "    start_epoch, best_val_loss, epochs_no_improve = 0, float('inf'), 0\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Resuming training from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        train_loss_history = checkpoint['train_loss_history']\n",
    "        val_loss_history = checkpoint['val_loss_history']\n",
    "        epochs_no_improve = checkpoint['epochs_no_improve']\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    else:\n",
    "        print(\"Starting new training run.\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping condition already met at loaded checkpoint (Patience: {epochs_no_improve}/{patience}).\")\n",
    "        print(\"Skipping training loop.\")\n",
    "        return model, train_loss_history, val_loss_history\n",
    "\n",
    "    best_model_weights = model.state_dict().copy()\n",
    "\n",
    "    # The training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [T]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            old_lr = optimizer.param_groups[0]['lr']\n",
    "            scheduler.step(avg_val_loss)\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            if new_lr < old_lr:\n",
    "                print(f\"  -> Learning rate reduced from {old_lr} to {new_lr}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss, epochs_no_improve = avg_val_loss, 0\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            print(\"  -> Validation loss improved!\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  -> No improvement. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_loss_history': train_loss_history, 'val_loss_history': val_loss_history,\n",
    "            'epochs_no_improve': epochs_no_improve,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"\\nEarly stopping triggered!\"); break\n",
    "\n",
    "    print(\"\\nTraining complete. Loading best model weights.\")\n",
    "    model.load_state_dict(best_model_weights)\n",
    "\n",
    "    return model, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "i3QI5AdWFfZK",
    "outputId": "eafa3e46-2d27-4f2c-eb30-71ec9e7ce36c"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_loader, device, all_results_dict):\n",
    "    \"\"\"\n",
    "    A master function to perform a complete evaluation (metrics and visualization)\n",
    "    on a trained model, print a detailed report, and store a curated summary.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Comprehensive Evaluation for: {model_name} ---\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=f\"Testing {model_name}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images); _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted.cpu().numpy().flatten())\n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "    all_preds, all_labels = np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "    # Calculate Metrics\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2])\n",
    "    overall_accuracy = 100 * np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
    "\n",
    "    class_names = [\"Non-Forest\", \"Forest\", \"Deforested\"]\n",
    "    precisions, recalls, f1_scores, ious = [], [], [], []\n",
    "\n",
    "    # Print Detailed Per-Class Report\n",
    "    print(\"\\n--- Detailed Per-Class Metrics ---\")\n",
    "    for i in range(len(class_names)):\n",
    "        TP = conf_matrix[i, i]; FP = np.sum(conf_matrix[:, i]) - TP; FN = np.sum(conf_matrix[i, :]) - TP\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "        precisions.append(precision); recalls.append(recall); f1_scores.append(f1); ious.append(iou)\n",
    "\n",
    "        print(f\"\\nClass: {class_names[i]} (Class {i})\")\n",
    "        print(f\"  - Precision: {precision:.3f}\")\n",
    "        print(f\"  - Recall:    {recall:.3f}\")\n",
    "        print(f\"  - F1-Score:  {f1:.3f}\")\n",
    "        print(f\"  - IoU:       {iou:.3f}\")\n",
    "\n",
    "    # Print Summary Metrics\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"--- Summary Metrics ---\")\n",
    "    print(f\"Overall Pixel Accuracy: {overall_accuracy:.2f}%\")\n",
    "    print(f\"Macro-Averaged F1-Score: {np.mean(f1_scores):.3f}\")\n",
    "    print(f\"Mean IoU (mIoU):         {np.mean(ious):.3f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Store Curated Results for Final Comparison Table\n",
    "    all_results_dict[model_name] = {\n",
    "        \"Overall Accuracy\": f\"{overall_accuracy:.2f}%\", \"Mean IoU (mIoU)\": f\"{np.mean(ious):.3f}\",\n",
    "        \"Macro F1-Score\": f\"{np.mean(f1_scores):.3f}\", \"F1: Non-Forest\": f\"{f1_scores[0]:.3f}\",\n",
    "        \"F1: Forest\": f\"{f1_scores[1]:.3f}\", \"F1: Deforested\": f\"{f1_scores[2]:.3f}\",\n",
    "        \"Recall: Deforested\": f\"{recalls[2]:.3f}\", \"Precision: Deforested\": f\"{precisions[2]:.3f}\",\n",
    "    }\n",
    "    print(f\"\\nCurated results for '{model_name}' have been stored for the final comparison table.\")\n",
    "\n",
    "    return all_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "RedTPPOaFjMp",
    "outputId": "2becc0d4-1bbc-44f8-993f-213e0f565e4c"
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(dataset, model, device, model_name, num_samples=3):\n",
    "    \"\"\"A master function to visualize model predictions on random samples.\"\"\"\n",
    "    print(f\"\\n--- Starting Qualitative Visualization for: {model_name} ---\")\n",
    "    color_map = np.array([[0, 0, 0], [0, 0.8, 0], [1, 0, 0]], dtype=np.float32)\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n",
    "    fig.suptitle(f'Prediction Analysis for: {model_name}', fontsize=16)\n",
    "    model.eval()\n",
    "    for i in range(num_samples):\n",
    "        sample_idx = random.randint(0, len(dataset) - 1)\n",
    "        image, label = dataset[sample_idx]\n",
    "        rgb_image = image.numpy()[:3, :, :].transpose(1, 2, 0)\n",
    "        p2, p98 = np.percentile(rgb_image, (2, 98)); rgb_image_stretched = np.clip((rgb_image - p2) / (p98 - p2), 0, 1)\n",
    "        label_viz = color_map[label.numpy()]\n",
    "        with torch.no_grad():\n",
    "            output = model(image.unsqueeze(0).to(device)); _, predicted = torch.max(output, 1)\n",
    "        predicted_viz = color_map[predicted.cpu().squeeze().numpy()]\n",
    "        axes[i, 0].imshow(rgb_image_stretched); axes[i, 0].set_title(f'Input (Sample #{sample_idx})'); axes[i, 0].axis('off')\n",
    "        axes[i, 1].imshow(label_viz); axes[i, 1].set_title('Ground Truth'); axes[i, 1].axis('off')\n",
    "        axes[i, 2].imshow(predicted_viz); axes[i, 2].set_title('Model Prediction'); axes[i, 2].axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdsvEoaJX_Is"
   },
   "source": [
    "---\n",
    "## 5. Model Training & Evaluation\n",
    "\n",
    "With our data prepared, we now proceed to train and evaluate our series of models. Each experiment is self-contained, calling our master `train_model` function and then immediately running a full analysis. We initialize a master dictionary, `all_results`, to programmatically store the key metrics from each run for our final comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "B-GUVGXlcAwk",
    "outputId": "59fb1e44-85ee-4244-cc56-9f06aefd7192"
   },
   "outputs": [],
   "source": [
    "# This dictionary will be populated by the analysis cell of each experiment.\n",
    "# We initialize it here to ensure it's created only once per session.\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NQH6Ae0cCbm"
   },
   "source": [
    "### 5.1. Experiment 1: Baseline FCN (Unweighted Loss)\n",
    "\n",
    "Our first experiment establishes a baseline. We will train the simple `SimpleFCN` model on our 4-channel static dataset using a standard, unweighted loss function. This will show us how a naive model performs and help us diagnose the core challenges of the dataset, such as class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3339ee286af14e2db79ccc5f145e5ca4",
      "a1f3d3eb961742b9bbec29662a00b0c5",
      "15b0f6a98de3455495f3cd9b2cbc5c01",
      "383c2f786e194017ba1d2bea02e37f1d",
      "1a5e7fc974f74beb9182556a33bc2a1a",
      "f28795d1299e47d9864dec861d92d843",
      "bd4f6c85ee7c4526b6ac5d1c0bce67aa",
      "4578f223ca214c739b61a2fdcd8299eb",
      "f4550afaa54b4b86a835418c9ff72f0a",
      "538fc8afc2534831b0a7d8e5054fc30f",
      "61944935f1694e52809e026627f5d0ed"
     ]
    },
    "id": "0TXRel0LcFcj",
    "outputId": "f19d33f8-59b6-4605-d1e5-a68725384c70"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- Starting Experiment 1: Baseline FCN ---\")\n",
    "\n",
    "# Define all components for this specific experiment\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(static_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(static_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "baseline_fcn = simpleFCN(in_channels=4, num_classes=3)\n",
    "baseline_optimizer = optim.Adam(baseline_fcn.parameters(), lr=0.001)\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX) # Use ignore_index even for unweighted\n",
    "baseline_chk_path = MODEL_DIR / 'baseline_fcn_checkpoint.pth'\n",
    "baseline_final_path = MODEL_DIR / 'best_baseline_fcn_model.pth'\n",
    "\n",
    "# Call the master training function\n",
    "baseline_fcn, baseline_train_hist, baseline_val_hist = train_model(\n",
    "    model=baseline_fcn,\n",
    "    criterion=baseline_criterion,\n",
    "    optimizer=baseline_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=None,\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    checkpoint_path=baseline_chk_path\n",
    ")\n",
    "\n",
    "# Save the final best model\n",
    "print(f\"\\nSaving final model to {baseline_final_path}\")\n",
    "torch.save(baseline_fcn.state_dict(), baseline_final_path)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# Analysis\n",
    "# Plot the loss curves for this experiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(baseline_train_hist, label='Training Loss')\n",
    "plt.plot(baseline_val_hist, label='Validation Loss')\n",
    "plt.title('Baseline FCN: Training & Validation Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and visualize the results\n",
    "test_loader = DataLoader(static_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_results = evaluate_model(\n",
    "    model=baseline_fcn,\n",
    "    model_name=\"Baseline FCN\",\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    all_results_dict=all_results\n",
    ")\n",
    "visualize_predictions(\n",
    "    dataset=static_test_dataset,\n",
    "    model=baseline_fcn,\n",
    "    device=device,\n",
    "    model_name=\"Baseline FCN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7pENfKALQAu"
   },
   "source": [
    "### 5.2. Experiment 2: Weighted FCN\n",
    "\n",
    "The baseline experiment confirmed that our dataset is highly imbalanced, causing the model to completely ignore the \"Deforested\" class. To solve this, our second experiment tests the effect of a **weighted loss function**.\n",
    "\n",
    "We will train the exact same `SimpleFCN` architecture, but this time, we will use a `WeightedCrossEntropyLoss` that heavily penalizes mistakes on the rare \"Deforested\" class. This allows us to scientifically isolate and measure the impact of this single change on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f45a631b02e94ff89e8accbeb1fb6825",
      "774def79f0ce41b1b86e8168f8e535a4",
      "6cc69e5dbd824c729979b9ecd9b17c68",
      "db62e114422e41e8acecb625b82767dd",
      "9230f0b3140a4e758bc1d8f37289b244",
      "c8ea3768149b4147b7cc78256cee5c42",
      "4bf285c58dbe4e429526b555971a6811",
      "29724a5f579d4e63a2128096d425a609",
      "ea62e2d51fab4962ad31340688a6bcda",
      "c0fc9055b53e4b4c9bc2f08255fe800c",
      "29634be7c1ca47b7a187a49d1756f2af"
     ]
    },
    "id": "aU76MxmVLRD3",
    "outputId": "cbc8010f-f923-4c31-99dd-6e3ae880ce0d"
   },
   "outputs": [],
   "source": [
    "# --- 5.2.1: Train and Analyze the Weighted FCN ---\n",
    "\n",
    "# Define all components for this specific experiment\n",
    "BATCH_SIZE = 64\n",
    "# We use the same static dataloaders as the baseline for a fair comparison\n",
    "train_loader = DataLoader(static_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(static_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "weighted_fcn = simpleFCN(in_channels=4, num_classes=3)\n",
    "weighted_fcn_optimizer = optim.Adam(weighted_fcn.parameters(), lr=0.001)\n",
    "\n",
    "# The key change for this experiment is using the pre-calculated class weights\n",
    "weighted_fcn_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=IGNORE_INDEX)\n",
    "\n",
    "# Use new, unique paths for this experiment's artifacts\n",
    "weighted_fcn_chk_path = MODEL_DIR / 'weighted_fcn_checkpoint.pth'\n",
    "weighted_fcn_final_path = MODEL_DIR / 'best_weighted_fcn_model.pth'\n",
    "\n",
    "# Call the master training function\n",
    "weighted_fcn, weighted_fcn_train_hist, weighted_fcn_val_hist = train_model(\n",
    "    model=weighted_fcn,\n",
    "    criterion=weighted_fcn_criterion,\n",
    "    optimizer=weighted_fcn_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=None,\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    checkpoint_path=weighted_fcn_chk_path\n",
    ")\n",
    "torch.save(weighted_fcn.state_dict(), weighted_fcn_final_path)\n",
    "print(f\"\\nFinal model for Experiment 2 saved to {weighted_fcn_final_path}\")\n",
    "\n",
    "# --- Analysis ---\n",
    "# Plot the loss curves for this experiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(weighted_fcn_train_hist, label='Training Loss')\n",
    "plt.plot(weighted_fcn_val_hist, label='Validation Loss')\n",
    "plt.title('Weighted FCN: Training & Validation Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and visualize the results\n",
    "test_loader = DataLoader(static_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_results = evaluate_model(\n",
    "    model=weighted_fcn,\n",
    "    model_name=\"Weighted FCN\",\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    all_results_dict=all_results\n",
    ")\n",
    "visualize_predictions(\n",
    "    dataset=static_test_dataset,\n",
    "    model=weighted_fcn,\n",
    "    device=device,\n",
    "    model_name=\"Weighted FCN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuZcADShQTL2"
   },
   "source": [
    "### 5.3. Experiment 3: Stable U-Net with Topographic Data\n",
    "\n",
    "Our first experiments showed that while weighted loss can force a simple model to see the rare \"Deforested\" class, the predictions are spatially noisy. To address this, we now introduce two major upgrades simultaneously:\n",
    "\n",
    "1.  **An Advanced Architecture:** We switch to the `U-Net`, whose encoder-decoder structure and skip connections are designed to produce much more precise, spatially coherent segmentation maps.\n",
    "2.  **Enriched Input Data:** We will train this model on our 7-band topographic dataset (R, G, B, NIR, Elevation, Slope) to test our hypothesis that this added geographical context can improve classification accuracy.\n",
    "\n",
    "To ensure the more complex U-Net trains effectively, we will also incorporate a `ReduceLROnPlateau` learning rate scheduler to automatically manage the learning rate and promote stable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5677fb1317e8499da90a0165f29d8ee9",
      "1246b1dfba8842beb62ba2cf96eaadc7",
      "caabfaf1fcfe4672a0b2269ad97c4721",
      "9865de1b4f6b48f791098bd5cc59e203",
      "fac03e2d7ff54c92af3a7a0f8afb21ce",
      "b5f66eb820e3444785c59c7a7378ac4d",
      "8ffbc6c8c0614e0c93399b711d2fab47",
      "35a425eba00d48699f42c79e790b5803",
      "e00ebd43a3234cc695ca4d5e24247c02",
      "15d2012918854a4ca3d8da28fc6ebdcf",
      "5e6888fee27e40d5b46d1e30ed6a513e"
     ]
    },
    "id": "3W7EIEqkQT9D",
    "outputId": "c4eb58d7-e148-43d7-9302-234f8528be81"
   },
   "outputs": [],
   "source": [
    "# Train and Analyze the Stable U-Net with Topo Data\n",
    "print(\"--- Starting Experiment 3: Stable U-Net with Topo Data ---\")\n",
    "\n",
    "# Define all components for this specific experiment\n",
    "BATCH_SIZE = 32 # U-Net is larger, so we use a smaller batch size\n",
    "# Use the TOPO datasets for this experiment\n",
    "train_loader = DataLoader(topo_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(topo_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Instantiate the UNet with 6 input channels for the topo data\n",
    "stable_unet = UNet(in_channels=6, num_classes=3)\n",
    "stable_unet_optimizer = optim.Adam(stable_unet.parameters(), lr=0.001)\n",
    "# We continue to use the weighted loss, as we've proven it's necessary\n",
    "stable_unet_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=IGNORE_INDEX)\n",
    "# Introduce the scheduler for this more complex model\n",
    "stable_unet_scheduler = ReduceLROnPlateau(stable_unet_optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "# Use new, unique paths for this experiment's artifacts\n",
    "stable_unet_chk_path = MODEL_DIR / 'topo_unet_checkpoint.pth'\n",
    "stable_unet_final_path = MODEL_DIR / 'best_topo_unet_model.pth'\n",
    "\n",
    "# Call the master training function\n",
    "stable_unet, stable_unet_train_hist, stable_unet_val_hist = train_model(\n",
    "    model=stable_unet,\n",
    "    criterion=stable_unet_criterion,\n",
    "    optimizer=stable_unet_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=stable_unet_scheduler, # Pass the scheduler\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    checkpoint_path=stable_unet_chk_path\n",
    ")\n",
    "torch.save(stable_unet.state_dict(), stable_unet_final_path)\n",
    "print(f\"\\nFinal model for Experiment 3 saved to {stable_unet_final_path}\")\n",
    "\n",
    "# --- Analysis ---\n",
    "# Plot the loss curves for this experiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(stable_unet_train_hist, label='Training Loss')\n",
    "plt.plot(stable_unet_val_hist, label='Validation Loss')\n",
    "plt.title('Stable U-Net (Topo): Training & Validation Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and visualize the results using the TOPO test set\n",
    "test_loader = DataLoader(topo_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_results = evaluate_model(\n",
    "    model=stable_unet,\n",
    "    model_name=\"Stable U-Net (Topo)\",\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    all_results_dict=all_results\n",
    ")\n",
    "visualize_predictions(\n",
    "    dataset=topo_test_dataset,\n",
    "    model=stable_unet,\n",
    "    device=device,\n",
    "    model_name=\"Stable U-Net (Topo)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVspaytOQXDv"
   },
   "source": [
    "### 5.4. Experiment 4: Enhancing Generalization with Data Augmentation\n",
    "\n",
    "Our previous experiments have yielded a strong `Stable U-Net` model trained on topographic data. As a final step to potentially improve its ability to generalize and make it more robust to variations in the input imagery, we introduce **data augmentation**.\n",
    "\n",
    "The core idea is to artificially expand our training set by applying random, realistic transformations to the image patches as they are loaded. This teaches the model to recognize features regardless of their orientation or minor lighting differences.\n",
    "\n",
    "#### 5.4.1. Defining the Augmented Dataset Pipeline\n",
    "\n",
    "To implement this, we first define an augmentation pipeline using the `Albumentations` library. This pipeline will apply random flips and rotations. We then create a new `AugmentedTopoDataset` class that integrates these transformations. Crucially, the augmentations are **only** applied to the training set; the validation and test sets remain unaltered to ensure a consistent and objective evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "N8hNemIFQY1C",
    "outputId": "a3ce4fca-f44d-4940-d12f-c77d86acf18d"
   },
   "outputs": [],
   "source": [
    "# Define and Create the Augmented Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_transforms(is_train):\n",
    "    \"\"\"Defines the augmentation pipeline.\"\"\"\n",
    "    # We use a simple normalization. For a production model, calculating\n",
    "    # the true mean/std of the dataset would be a further improvement.\n",
    "    mean_std = [0.5] * 6 # For our 6 input channels\n",
    "\n",
    "    if is_train:\n",
    "        # Augmentation pipeline for the training set.\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Normalize(mean=mean_std, std=mean_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        # Validation/test set only needs normalization and tensor conversion.\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=mean_std, std=mean_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "class AugmentedTopoDataset(Dataset):\n",
    "    \"\"\"A Dataset class for the 7-band topo data that applies Albumentations transforms.\"\"\"\n",
    "    def __init__(self, root_dir, transform=None, target_size=64):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.tif')]\n",
    "\n",
    "    def __len__(self): return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        with rasterio.open(img_path) as dataset:\n",
    "            numpy_array = dataset.read(masked=True).filled(fill_value=IGNORE_INDEX)\n",
    "\n",
    "        _, h, w = numpy_array.shape\n",
    "        if h > self.target_size or w > self.target_size:\n",
    "            top = (h - self.target_size) // 2; left = (w - self.target_size) // 2\n",
    "            numpy_array = numpy_array[:, top:top + self.target_size, left:left + self.target_size]\n",
    "\n",
    "        # Albumentations expects image in (H, W, C) format\n",
    "        image_numpy = numpy_array[:6, :, :].transpose(1, 2, 0)\n",
    "        label_numpy = numpy_array[6, :, :]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image_numpy, mask=label_numpy)\n",
    "            image_tensor = augmented['image']\n",
    "            label_tensor = augmented['mask'].to(torch.long)\n",
    "        else: # Fallback\n",
    "            image_tensor = torch.from_numpy(image_numpy.transpose(2, 0, 1)).to(torch.float32)\n",
    "            label_tensor = torch.from_numpy(label_numpy).to(torch.long)\n",
    "\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "# --- Create new datasets with the appropriate transforms ---\n",
    "TOPO_FOLDER = DATA_DIR / \"sat_patch_topo\"\n",
    "# We need to re-split to ensure the correct transforms are applied to the right sets\n",
    "aug_train_full = AugmentedTopoDataset(root_dir=TOPO_FOLDER, transform=get_transforms(is_train=True))\n",
    "aug_val_test_full = AugmentedTopoDataset(root_dir=TOPO_FOLDER, transform=get_transforms(is_train=False))\n",
    "\n",
    "train_size = int(0.7 * len(aug_train_full))\n",
    "val_size = int(0.15 * len(aug_train_full))\n",
    "test_size = len(aug_train_full) - train_size - val_size\n",
    "\n",
    "# The training set is a subset of the augmented data\n",
    "aug_train_dataset, _ = random_split(aug_train_full, [train_size, val_size + test_size], generator=torch.Generator().manual_seed(42))\n",
    "# The val and test sets are subsets of the non-augmented data\n",
    "_, aug_val_dataset, aug_test_dataset = random_split(aug_val_test_full, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(\"Created new datasets with data augmentation pipeline integrated.\")\n",
    "print(f\"Total samples: Train ({len(aug_train_dataset)}), Val ({len(aug_val_dataset)}), Test ({len(aug_test_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vkdkdYiRd-U"
   },
   "source": [
    "#### 5.4.2. Defining the Augmented Dataset Pipeline\n",
    "\n",
    "To implement data augmentation, we first define the pipeline of transformations using the `Albumentations` library. We then create a new `AugmentedTopoDataset` class which integrates this pipeline.\n",
    "\n",
    "A crucial step in this process is re-splitting the data. We create two versions of the full dataset: one with the training augmentations applied and one without. The final `aug_train_dataset` is taken from the augmented version, while the `aug_val_dataset` and `aug_test_dataset` are taken from the non-augmented version. This ensures that our model is trained on a varied, augmented set but is always validated and tested against a consistent, unaltered set of images, guaranteeing a fair and reproducible evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f0ffc793968a43ada34ec489f29d758c",
      "f127303040de41258577ffd85e659a06",
      "464b6b7c79724159bf7d9431de97b97e",
      "23f15b3116cd4982abacdcda562310ae",
      "3e7d46338b1c41a5bccfdea43b1af5b1",
      "c455dd181ee046bfaf2c525843de9e82",
      "d27f58e1a4434890a22bdd20f13510e0",
      "825585ff0a4a46deb73a2beae46cb560",
      "786d38fe97034184bd53fa1692bdbecc",
      "f38fc02fb58e4f148ee5e14c1aa9bebd",
      "04b2c0a27ff84aed97f8d40bdd4880e4"
     ]
    },
    "id": "XdOASclIQj3B",
    "outputId": "5f3e0664-35d1-4f4f-d8e2-5f6c8b9b3813"
   },
   "outputs": [],
   "source": [
    "# Train and Analyze the Augmented Topo-UNet\n",
    "print(\"--- Starting Experiment 4: Augmented Topo-UNet ---\")\n",
    "\n",
    "# Define components using the new augmented datasets\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(aug_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(aug_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "aug_unet_model = UNet(in_channels=6, num_classes=3)\n",
    "aug_unet_optimizer = optim.Adam(aug_unet_model.parameters(), lr=0.001)\n",
    "aug_unet_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=IGNORE_INDEX)\n",
    "aug_unet_scheduler = ReduceLROnPlateau(aug_unet_optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "aug_unet_chk_path = MODEL_DIR / 'aug_unet_checkpoint.pth'\n",
    "aug_unet_final_path = MODEL_DIR / 'best_aug_unet_model.pth'\n",
    "\n",
    "# Call the master training function\n",
    "aug_unet_model, aug_train_hist, aug_val_hist = train_model(\n",
    "    model=aug_unet_model,\n",
    "    criterion=aug_unet_criterion,\n",
    "    optimizer=aug_unet_optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=aug_unet_scheduler,\n",
    "    num_epochs=25,\n",
    "    patience=5,\n",
    "    checkpoint_path=aug_unet_chk_path\n",
    ")\n",
    "torch.save(aug_unet_model.state_dict(), aug_unet_final_path)\n",
    "print(f\"\\nFinal model for Experiment 4 saved to {aug_unet_final_path}\")\n",
    "\n",
    "# --- Analysis ---\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(aug_train_hist, label='Training Loss')\n",
    "plt.plot(aug_val_hist, label='Validation Loss')\n",
    "plt.title('Augmented Topo-UNet: Training & Validation Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and visualize the results\n",
    "test_loader = DataLoader(aug_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_results = evaluate_model(\n",
    "    model=aug_unet_model,\n",
    "    model_name=\"Augmented Topo-UNet\",\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    all_results_dict=all_results\n",
    ")\n",
    "visualize_predictions(\n",
    "    dataset=aug_test_dataset,\n",
    "    model=aug_unet_model,\n",
    "    device=device,\n",
    "    model_name=\"Augmented Topo-UNet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVWq513cQjO8"
   },
   "source": [
    "---\n",
    "## 6. Final Analysis and Comparison\n",
    "\n",
    "With all our experiments complete, we can now directly compare the performance of each model to understand the impact of our different strategies. This final analysis will allow us to declare a \"winning\" model and summarize the key learnings of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4OcetUVRjcm"
   },
   "source": [
    "\n",
    "### 6.1. Quantitative Comparison\n",
    "\n",
    "We will now generate a dynamic summary table from the `all_results` dictionary that we populated during our experiments. This table provides a clean, head-to-head comparison of the most important quantitative metrics for each model, allowing us to objectively assess their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "LGasabKf1Iwc",
    "outputId": "d250b05b-db59-468c-c842-8a870c204b4d"
   },
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Head-to-Head Model Comparison ---\")\n",
    "\n",
    "# Ensure all_results exists. If not, initialize it.\n",
    "if 'all_results' not in locals():\n",
    "    all_results = {}\n",
    "\n",
    "# Define all models and their required information\n",
    "models_to_evaluate = {\n",
    "    \"Baseline FCN\": {\n",
    "        \"class\": simpleFCN, \"params\": {\"in_channels\": 4, \"num_classes\": 3},\n",
    "        \"path\": MODEL_DIR / 'best_baseline_fcn_model.pth',\n",
    "        \"dataset\": static_test_dataset\n",
    "    },\n",
    "    \"Weighted FCN\": {\n",
    "        \"class\": simpleFCN, \"params\": {\"in_channels\": 4, \"num_classes\": 3},\n",
    "        \"path\": MODEL_DIR / 'best_weighted_fcn_model.pth',\n",
    "        \"dataset\": static_test_dataset\n",
    "    },\n",
    "    \"Stable U-Net (Topo)\": {\n",
    "        \"class\": UNet, \"params\": {\"in_channels\": 6, \"num_classes\": 3},\n",
    "        \"path\": MODEL_DIR / 'best_topo_unet_model.pth',\n",
    "        \"dataset\": topo_test_dataset\n",
    "    },\n",
    "    \"Augmented Topo-UNet\": {\n",
    "        \"class\": UNet, \"params\": {\"in_channels\": 6, \"num_classes\": 3},\n",
    "        \"path\": MODEL_DIR / 'best_aug_unet_model.pth',\n",
    "        \"dataset\": aug_test_dataset\n",
    "    }\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loop through and evaluate any models whose results aren't already in all_results\n",
    "for name, info in models_to_evaluate.items():\n",
    "    if name not in all_results:\n",
    "        print(f\"Results for '{name}' not in memory. Evaluating from saved file...\")\n",
    "        if os.path.exists(info['path']):\n",
    "            model = info['class'](**info['params'])\n",
    "            model.load_state_dict(torch.load(info['path'], map_location=device))\n",
    "            model.to(device)\n",
    "            test_loader = DataLoader(info['dataset'], batch_size=32, shuffle=False)\n",
    "            all_results = evaluate_model(model, name, test_loader, device, all_results)\n",
    "        else:\n",
    "            print(f\"Model file for {name} not found. Skipping.\")\n",
    "\n",
    "# Generate and display the final table\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    metric_order = [\"Overall Accuracy\", \"Mean IoU (mIoU)\", \"Macro F1-Score\", \"F1: Non-Forest\",\n",
    "                    \"F1: Forest\", \"F1: Deforested\", \"Recall: Deforested\", \"Precision: Deforested\"]\n",
    "    column_order = [\"Baseline FCN\", \"Weighted FCN\", \"Stable U-Net (Topo)\", \"Augmented Topo-UNet\"]\n",
    "    final_column_order = [col for col in column_order if col in results_df.columns]\n",
    "    results_df = results_df.reindex(metric_order)[final_column_order]\n",
    "    display(Markdown(results_df.to_markdown()))\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XGSqSSxRnfa"
   },
   "source": [
    "### 6.2. Qualitative Comparison\n",
    "\n",
    "While the metrics table provides the numbers, a visual comparison provides the most intuitive understanding of how the models' behaviors evolved. The following script loads the best saved weights for each of our trained models and generates a side-by-side prediction for the exact same input images, allowing for a direct, qualitative comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "id": "meosbrh0RovX",
    "outputId": "8f7c9ab7-f6ff-4e80-f2a0-dbb8498c09bf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Define the models and the CORRECT datasets they were trained on ---\n",
    "models_to_visualize = {\n",
    "    \"Baseline FCN\": {\n",
    "        \"path\": MODEL_DIR / 'best_baseline_fcn_model.pth',\n",
    "        \"architecture\": simpleFCN(in_channels=4, num_classes=3),\n",
    "        \"dataset\": static_test_dataset\n",
    "    },\n",
    "    \"Weighted FCN\": {\n",
    "        \"path\": MODEL_DIR / 'best_weighted_fcn_model.pth',\n",
    "        \"architecture\": simpleFCN(in_channels=4, num_classes=3),\n",
    "        \"dataset\": static_test_dataset\n",
    "    },\n",
    "    \"Stable U-Net (Topo)\": {\n",
    "        \"path\": MODEL_DIR / 'best_topo_unet_model.pth',\n",
    "        \"architecture\": UNet(in_channels=6, num_classes=3),\n",
    "        \"dataset\": topo_test_dataset\n",
    "    },\n",
    "    \"Augmented Topo-UNet\": {\n",
    "        \"path\": MODEL_DIR / 'best_aug_unet_model.pth',\n",
    "        \"architecture\": UNet(in_channels=6, num_classes=3),\n",
    "        \"dataset\": aug_test_dataset\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 2. Load all the models into a dictionary ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loaded_models_for_viz = {}\n",
    "for name, info in models_to_visualize.items():\n",
    "    if os.path.exists(info['path']):\n",
    "        model = info['architecture']\n",
    "        model.load_state_dict(torch.load(info['path'], map_location=device))\n",
    "        model.to(device)\n",
    "        loaded_models_for_viz[name] = model\n",
    "    else:\n",
    "        print(f\"Warning: Model file for '{name}' not found. It will be excluded.\")\n",
    "\n",
    "# --- 3. Run the side-by-side visualization ---\n",
    "if loaded_models_for_viz:\n",
    "    def visualize_final_comparison(models_dict, num_samples=3):\n",
    "        color_map = np.array([[0, 0, 0], [0, 0.8, 0], [1, 0, 0]], dtype=np.float32)\n",
    "        num_cols = len(models_dict) + 2\n",
    "        fig, axes = plt.subplots(num_samples, num_cols, figsize=(num_cols * 4, num_samples * 4.5))\n",
    "        fig.suptitle('Side-by-Side Model Prediction Comparison', fontsize=20)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # We sample an index, then get the corresponding data from each dataset\n",
    "            # We'll use the final dataset's length for the random index\n",
    "            sample_idx = random.randint(0, len(aug_test_dataset) - 1)\n",
    "\n",
    "            # Get the correct input image for the plot header (we'll use the topo version)\n",
    "            image_for_plot, label_for_plot = models_to_visualize['Stable U-Net (Topo)']['dataset'][sample_idx]\n",
    "            rgb_image = image_for_plot.numpy()[:3, :, :].transpose(1, 2, 0)\n",
    "            p2, p98 = np.percentile(rgb_image, (2, 98)); rgb_image_stretched = np.clip((rgb_image - p2) / (p98 - p2), 0, 1)\n",
    "            label_viz = color_map[label_for_plot.numpy()]\n",
    "            axes[i, 0].imshow(rgb_image_stretched); axes[i, 0].set_title(f'Input (Sample #{sample_idx})'); axes[i, 0].axis('off')\n",
    "            axes[i, 1].imshow(label_viz); axes[i, 1].set_title('Ground Truth'); axes[i, 1].axis('off')\n",
    "\n",
    "            # Loop through models to generate predictions\n",
    "            for j, (model_name, model) in enumerate(models_dict.items()):\n",
    "                ax = axes[i, j + 2]\n",
    "                model.eval()\n",
    "\n",
    "                # --- THE FIX IN ACTION ---\n",
    "                # Get the correct dataset for the current model\n",
    "                current_dataset = models_to_visualize[model_name]['dataset']\n",
    "                # Get the correct image from that dataset\n",
    "                image_to_predict, _ = current_dataset[sample_idx]\n",
    "                input_tensor = image_to_predict.unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor); _, predicted = torch.max(output, 1)\n",
    "                predicted_viz = color_map[predicted.cpu().squeeze().numpy()]\n",
    "                ax.imshow(predicted_viz); ax.set_title(model_name); ax.axis('off')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.96]); plt.show()\n",
    "\n",
    "    visualize_final_comparison(loaded_models_for_viz)\n",
    "else:\n",
    "    print(\"No models were loaded for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG0a561hr9hF"
   },
   "source": [
    "### Analysis of Comparative Results\n",
    "\n",
    "The head-to-head comparison of our four experimental models tells a clear and compelling story of iterative improvement, trade-offs, and the challenges inherent in this geospatial task.\n",
    "\n",
    "**Act 1: The Failure of the Naive Baseline**\n",
    "The `Baseline FCN` achieved the highest top-line metrics, with an **Overall Accuracy of 87.00%** and a **Macro F1-Score of 0.571**. However, these numbers are deeply misleading. A closer look reveals a **Recall and F1-Score of 0.000** for the \"Deforested\" class. The model learned to achieve a high score by completely ignoring the rare class, making it a failure for our specific problem statement. This result definitively proves that class imbalance is the primary challenge that must be addressed.\n",
    "\n",
    "**Act 2: Solving Class Imbalance with Weighted Loss**\n",
    "The `Weighted FCN` represents our first major breakthrough. By introducing a weighted loss function, the **Recall for \"Deforested\" skyrocketed from 0% to 54.2%**. This proves that the model *can* be forced to learn the minority class. However, this came at a significant cost: the **Precision for \"Deforested\" was an abysmal 0.9%**, and the overall performance (mIoU, Macro F1) dropped as the model became overly aggressive in its predictions. The visualizations for this model also revealed spatially noisy, \"speckled\" outputs, indicating architectural limitations.\n",
    "\n",
    "**Act 3: The Architectural Leap with the U-Net**\n",
    "The `Stable U-Net (Topo)` immediately demonstrated the power of a superior architecture. It produced visually clean, spatially coherent maps and successfully balanced performance across the majority classes, achieving a strong **F1-Score of 0.842 for Non-Forest** and **0.819 for Forest**. It also improved the **Precision for \"Deforested\" by 44%** over the Weighted FCN. This experiment proved that the U-Net, enhanced with topographic data and stabilized with an LR scheduler, is a fundamentally better-suited architecture for this segmentation task.\n",
    "\n",
    "**Act 4: The Final Polish with Augmentation**\n",
    "The final `Augmented Topo-UNet` provided a marginal but measurable improvement over its non-augmented counterpart, nudging the **Macro F1-Score to 0.557** and boosting the **Recall for \"Deforested\" to 54.3%**. This demonstrates the value of data augmentation as a fine-tuning technique for improving model generalization. However, it did not solve the core, persistent challenge.\n",
    "\n",
    "**The Unsolved Problem: Low Precision**\n",
    "Across all experiments, the most stubborn problem was the extremely low precision for the \"Deforested\" class. Even our best model, while successfully identifying over half of the true deforestation, still produces a high rate of false alarms, confusing other spectrally similar features (like riverbeds or bare soil) with true deforestation. This suggests we have reached the limits of what is possible with a single-snapshot, static image approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAFi1foCsADz"
   },
   "source": [
    "---\n",
    "## 7. Project Conclusion, Limitations, and Future Work\n",
    "\n",
    "### 7.1. Conclusion\n",
    "\n",
    "This project successfully developed and rigorously evaluated an end-to-end pipeline for a challenging semantic segmentation task: mapping deforestation in Nepal from satellite imagery. Through a series of methodical experiments, we demonstrated a clear path of model improvement:\n",
    "\n",
    "1.  We began by proving that a naive baseline model fails completely on the imbalanced \"Deforested\" class.\n",
    "2.  We then proved that a **`WeightedCrossEntropyLoss`** is an essential and effective strategy to force the model to learn this rare class, dramatically improving its **Recall**.\n",
    "3.  We demonstrated that a more advanced **`U-Net` architecture**, enhanced with **topographic data**, produces architecturally superior, more balanced, and more accurate predictions than a simple FCN.\n",
    "4.  Finally, we showed that **data augmentation** provides a marginal but positive final boost to generalization.\n",
    "\n",
    "Our winning model, the **`Augmented Topo-UNet`**, represents the pinnacle of this single-image, static classification approach. However, our analysis also definitively concluded that this approach has a fundamental limitation.\n",
    "\n",
    "### 7.2. Limitations\n",
    "\n",
    "The primary limitation of this project, which persisted across all models, is the **low precision for the \"Deforested\" class**. This is not a failure of the models themselves, but a fundamental challenge rooted in the data and the problem formulation:\n",
    "\n",
    "*   **Spectral Ambiguity:** A single-snapshot, 10-meter resolution satellite image lacks the context to reliably distinguish the spectral signature of recently cleared land from other naturally occurring bright features like sand, riverbeds, or specific types of soil. A model trained on this data will inevitably produce a high rate of false positives.\n",
    "*   **Lack of Temporal Context:** The core nature of deforestation is **change over time**. By training a model on a single static image to predict a historical state, we are asking it to infer a temporal process from static evidence, which is an inherently difficult and ambiguous task.\n",
    "\n",
    "### 7.3. Future Work\n",
    "\n",
    "Based on the clear limitations identified in this study, the following avenues represent the most promising directions for future research to achieve a truly operational deforestation monitoring system:\n",
    "\n",
    "1.  **Temporal Change Detection (Highest Priority):** The most critical next step is to reframe the problem from static scene classification to **temporal change detection**. This would involve:\n",
    "    *   Creating a new dataset where the input `X` is a **12-channel tensor** containing two stacked 6-channel images from different time periods (e.g., 2022 and 2023).\n",
    "    *   Creating a new binary label `y` representing **change vs. no change** within that specific time interval.\n",
    "    *   Training a U-Net on this data to learn the direct signature of change. This approach is hypothesized to dramatically increase precision by learning to ignore features that are static (like riverbeds).\n",
    "\n",
    "2.  **Advanced Architectures (e.g., Vision Transformer):** To further improve contextual understanding, a Transformer-based segmentation architecture like **SegFormer** could be implemented. Its self-attention mechanism is ideally suited to learn the long-range spatial relationships needed to distinguish a long, winding river from a small, isolated deforestation patch.\n",
    "\n",
    "3.  **Post-Processing with Conditional Random Fields (CRFs):** A CRF could be applied as a final \"clean-up\" step to the U-Net's output. By considering the spatial relationships of the predicted pixels, a CRF can help to remove small, isolated \"false positive\" predictions and produce even cleaner, more realistic segmentation maps."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
